{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Plane Adoption procedure","text":""},{"location":"#openstack-adoption","title":"OpenStack adoption","text":"<p>This is a procedure for adopting an OpenStack cloud.</p> <p>Perform the actions from the sub-documents in the following order:</p> <ul> <li> <p>Deploy podified backend services</p> </li> <li> <p>Copy MariaDB data</p> </li> <li> <p>Keystone adoption</p> </li> <li> <p>Glance adoption</p> </li> <li> <p>Adoption of other services</p> </li> </ul> <p>If you face issues during adoption, check the Troubleshooting document for common problems and solutions.</p>"},{"location":"#post-openstack-ceph-adoption","title":"Post-OpenStack Ceph adoption","text":"<p>If the environment includes Ceph and some of its services are collocated on the Controller hosts (\"internal Ceph\"), then Ceph services need to be moved out of Controller hosts as the last step of the OpenStack adoption. Follow this documentation:</p> <ul> <li>Ceph RBD migration</li> <li>Ceph RGW migration</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>For information about contributing to the docs and how to run tests, see:</p> <ul> <li> <p>Contributing to documentation -   how to build docs locally, docs patterns and tips.</p> </li> <li> <p>Tests -   information about the test suite and how to run it.</p> </li> </ul>"},{"location":"ceph/ceph_rbd/","title":"Data Plane adoption - Ceph RBD Migration","text":"<p>In this scenario, assuming Ceph is already &gt;= 5, either for HCI or dedicated Storage nodes, the daemons living in the OpenStack control plane should be moved/migrated into the existing external RHEL nodes (typically the compute nodes for an HCI environment or dedicated storage nodes in all the remaining use cases).</p>"},{"location":"ceph/ceph_rbd/#requirements","title":"Requirements","text":"<ul> <li>Ceph is &gt;= 5 and managed by cephadm/orchestrator</li> <li>Ceph NFS (ganesha) migrated from a TripleO based deployment to cephadm</li> <li>Both the Ceph public and cluster networks are propagated, via TripleO, to the target nodes</li> <li>Ceph Mons need to keep their IPs (to avoid cold migration).</li> </ul>"},{"location":"ceph/ceph_rbd/#scenario-1-migrate-mon-and-mgr-from-controller-nodes","title":"SCENARIO 1: Migrate mon and mgr from controller nodes","text":"<p>The goal of the first POC is to prove we are able to successfully drain a controller node, in terms of ceph daemons, and move them to a different node. The initial target of the POC is RBD only, which means we\u2019re going to move only mon and mgr daemons. For the purposes of this POC, we'll deploy a ceph cluster with only mon, mgrs, and osds to simulate the environment a customer will be in before starting the migration. The goal of the first POC is to ensure that: - We can keep the mon IP addresses moving them to the CephStorage nodes - We can drain the existing controller nodes and shutting them down - We can deploy additional monitors to the existing nodes, promoting them as   _admin nodes that can be used by administrators to manage the ceph cluster   and perform day2 operations against it - We can keep the cluster operational during the migration</p>"},{"location":"ceph/ceph_rbd/#prerequisites","title":"Prerequisites","text":"<p>The Storage Nodes should be configured to have both storage and storage_mgmt network to make sure we can use both Ceph public and cluster networks.</p> <p>This step is the only one where the interaction with TripleO is required. From 17+ we don\u2019t have to run any stack update, however, we have commands that should be performed to run os-net-config on the baremetal node and configure additional networks.</p> <p>Make sure the network is defined in metalsmith.yaml for the CephStorageNodes:</p> <pre><code>- name: CephStorage\n  count: 2\n  instances:\n    - hostname: oc0-ceph-0\n      name: oc0-ceph-0\n    - hostname: oc0-ceph-1\n      name: oc0-ceph-1\n  defaults:\n    networks:\n      - network: ctlplane\n        vif: true\n      - network: storage_cloud_0\n          subnet: storage_cloud_0_subnet\n      - network: storage_mgmt_cloud_0\n          subnet: storage_mgmt_cloud_0_subnet\n    network_config:\n      template: templates/single_nic_vlans/single_nic_vlans_storage.j2\n</code></pre> <p>Then run:</p> <pre><code>openstack overcloud node provision \\\n  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \\\n  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam\n</code></pre> <p>Verify that the storage network is running on the node:</p> <pre><code>(undercloud) [CentOS-9 - stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a\nWarning: Permanently added '192.168.24.14' (ED25519) to the list of known hosts.\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"ceph/ceph_rbd/#migrate-mons-and-mgrs-on-the-two-existing-cephstorage-nodes","title":"Migrate mon(s) and mgr(s) on the two existing CephStorage nodes","text":"<p>Create a ceph spec based on the default roles with the mon/mgr on the controller nodes.</p> <pre><code>openstack overcloud ceph spec -o ceph_spec.yaml -y  \\\n   --stack overcloud-0     overcloud-baremetal-deployed-0.yaml\n</code></pre> <p>Deploy the Ceph cluster</p> <pre><code> openstack overcloud ceph deploy overcloud-baremetal-deployed-0.yaml \\\n    --stack overcloud-0 -o deployed_ceph.yaml \\\n    --network-data ~/oc0-network-data.yaml \\\n    --ceph-spec ~/ceph_spec.yaml\n</code></pre> <p>Note:</p> <p>The ceph_spec.yaml, which is the OSP generated description of the ceph cluster, will be used, later in the process, as the basic template required by cephadm to update the status/info of the daemons</p> <p>Check the status of the cluster</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2 (age 19m)\n    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk\n    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  _admin mgr mon\noc0-controller-1  192.168.24.23  _admin mgr mon\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>The goal of the next section is to migrate the oc0-controller-{1,2} daemons into oc0-ceph-{0,1} as the very basic scenario that demonstrates we can actually make this kind of migration using cephadm.</p>"},{"location":"ceph/ceph_rbd/#migrate-oc0-controller-1-into-oc0-ceph-0","title":"Migrate oc0-controller-1 into oc0-ceph-0","text":"<p>ssh into controller-0, then</p> <p><code>cephadm shell -v /home/ceph-admin/specs:/specs</code></p> <p>ssh into ceph-0, then</p> <p><code>sudo \u201cwatch podman ps\u201d  # watch the new mon/mgr being deployed here</code></p> <p>(optional) if mgr is active in the source node, then:</p> <pre><code>ceph mgr fail &lt;mgr instance&gt;\n</code></pre> <p>From the cephadm shell, remove the labels on oc0-controller-1</p> <pre><code>    for label in mon mgr _admin; do\n           ceph orch host rm label oc0-controller-1 $label;\n    done\n</code></pre> <p>Add the missing labels to oc0-ceph-0</p> <pre><code>[ceph: root@oc0-controller-0 /]#\n&gt; for label in mon mgr _admin; do ceph orch host label add oc0-ceph-0 $label; done\nAdded label mon to host oc0-ceph-0\nAdded label mgr to host oc0-ceph-0\nAdded label _admin to host oc0-ceph-0\n</code></pre> <p>Drain and force-remove the oc0-controller-1 node</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host drain oc0-controller-1\nScheduled to remove the following daemons from host 'oc0-controller-1'\ntype                 id\n-------------------- ---------------\nmon                  oc0-controller-1\nmgr                  oc0-controller-1.mtxohd\ncrash                oc0-controller-1\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host rm oc0-controller-1 --force\nRemoved  host 'oc0-controller-1'\n\n[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  mgr mon _admin\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>If you have only 3 mon nodes, and the drain of the node doesn\u2019t work as expected (the containers are still there), then SSH to controller-1 and force-purge the containers in the node:</p> <pre><code>[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES\n5c1ad36472bc  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1\n3b14cc7bf4dd  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd\n\n[root@oc0-controller-1 ~]# cephadm rm-cluster --fsid f6ec3ebe-26f7-56c8-985d-eb974e8e08e3 --force\n\n[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES\n</code></pre> <p>Note: cephadm rm-cluster on a node which is not part of the cluster anymore has the effect of removing all the containers and doing some cleanup on the filesystem.</p> <p>Before shutting the oc0-controller-1 down, move the ip address (on the same network) to the oc0-ceph-0 node:</p> <pre><code>mon_host = [v2:172.16.11.54:3300/0,v1:172.16.11.54:6789/0] [v2:172.16.11.121:3300/0,v1:172.16.11.121:6789/0] [v2:172.16.11.205:3300/0,v1:172.16.11.205:6789/0]\n\n[root@oc0-controller-1 ~]# ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-ex    inet 192.168.24.23/24 brd 192.168.24.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n6: vlan100    inet 192.168.100.96/24 brd 192.168.100.255 scope global vlan100\\       valid_lft forever preferred_lft forever\n7: vlan12    inet 172.16.12.154/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n8: vlan11    inet 172.16.11.121/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n9: vlan13    inet 172.16.13.178/24 brd 172.16.13.255 scope global vlan13\\       valid_lft forever preferred_lft forever\n10: vlan70    inet 172.17.0.23/20 brd 172.17.15.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n11: vlan1    inet 192.168.24.23/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n12: vlan14    inet 172.16.14.223/24 brd 172.16.14.255 scope global vlan14\\       valid_lft forever preferred_lft forever\n</code></pre> <p>On the oc0-ceph-0:</p> <pre><code>[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n[heat-admin@oc0-ceph-0 ~]$ sudo ip a add 172.16.11.121 dev vlan11\n[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.121/32 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre> <p>Poweroff oc0-controller-1.</p> <p>Add the new mon on oc0-ceph-0 using the old ip address:</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch daemon add mon oc0-ceph-0:172.16.11.121\nDeployed mon.oc0-ceph-0 on host 'oc0-ceph-0'\n</code></pre> <p>Check the new container in the oc0-ceph-0 node:</p> <pre><code>b581dc8bbb78  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-ceph-0...  24 seconds ago  Up 24 seconds ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-ceph-0\n</code></pre> <p>On the cephadm shell, backup the existing ceph_spec.yaml, edit the spec removing any oc0-controller-1 entry, and replace it with oc0-ceph-0:</p> <pre><code>cp ceph_spec.yaml ceph_spec.yaml.bkp # backup the ceph_spec.yaml file\n\n[ceph: root@oc0-controller-0 specs]# diff -u ceph_spec.yaml.bkp ceph_spec.yaml\n\n--- ceph_spec.yaml.bkp  2022-07-29 15:41:34.516329643 +0000\n+++ ceph_spec.yaml      2022-07-29 15:28:26.455329643 +0000\n@@ -7,14 +7,6 @@\n - mgr\n service_type: host\n ---\n-addr: 192.168.24.12\n-hostname: oc0-controller-1\n-labels:\n-- _admin\n-- mon\n-- mgr\n-service_type: host\n----\n addr: 192.168.24.19\n hostname: oc0-controller-2\n labels:\n@@ -38,7 +30,7 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n+  - oc0-ceph-0\n   - oc0-controller-2\n service_id: mon\n service_name: mon\n@@ -47,8 +39,8 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n   - oc0-controller-2\n+  - oc0-ceph-0\n service_id: mgr\n service_name: mgr\n service_type: mgr\n</code></pre> <p>Apply the resulting spec:</p> <pre><code>ceph orch apply -i ceph_spec.yaml \n\n The result of 12 is having a new mgr deployed on the oc0-ceph-0 node, and the spec reconciled within cephadm\n\n[ceph: root@oc0-controller-0 specs]# ceph orch ls\nNAME                     PORTS  RUNNING  REFRESHED  AGE  PLACEMENT\ncrash                               4/4  5m ago     61m  *\nmgr                                 3/3  5m ago     69s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nmon                                 3/3  5m ago     70s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nosd.default_drive_group               8  2m ago     69s  oc0-ceph-0;oc0-ceph-1\n\n[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_WARN\n            1 stray host(s) with 1 daemon(s) not managed by cephadm\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 5m)\n    mgr: oc0-controller-0.xzgtvo(active, since 62m), standbys: oc0-controller-2.ahrgsk, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 42m), 8 in (since 49m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>Fix the warning by refreshing the mgr:</p> <pre><code>ceph mgr fail oc0-controller-0.xzgtvo\n</code></pre> <p>And at this point the cluster is clean:</p> <pre><code>[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 7m)\n    mgr: oc0-controller-2.ahrgsk(active, since 25s), standbys: oc0-controller-0.xzgtvo, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 44m), 8 in (since 50m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>oc0-controller-1 has been removed and powered off without leaving traces on the ceph cluster.</p> <p>The same approach and the same steps can be applied to migrate oc0-controller-2 to oc0-ceph-1.</p>"},{"location":"ceph/ceph_rbd/#screen-recording","title":"Screen Recording:","text":"<ul> <li>Externalize a TripleO deployed Ceph cluster</li> </ul>"},{"location":"ceph/ceph_rbd/#whats-next","title":"What\u2019s next","text":""},{"location":"ceph/ceph_rbd/#useful-resources","title":"Useful resources","text":"<ul> <li>cephadm - deploy additional mon(s)</li> </ul>"},{"location":"ceph/ceph_rgw/","title":"Data Plane adoption - Ceph RGW Migration","text":"<p>In this scenario, assuming Ceph is already &gt;= 5, either for HCI or dedicated Storage nodes, the RGW daemons living in the OpenStack Controller nodes will be migrated into the existing external RHEL nodes (typically the Compute nodes for an HCI environment or CephStorage nodes in the remaining use cases).</p>"},{"location":"ceph/ceph_rgw/#requirements","title":"Requirements","text":"<ul> <li>Ceph is &gt;= 5 and managed by cephadm/orchestrator</li> <li>An undercloud is still available: nodes and networks are managed by TripleO</li> </ul>"},{"location":"ceph/ceph_rgw/#ceph-daemon-cardinality","title":"Ceph Daemon Cardinality","text":"<p>Ceph 5+ applies strict constraints in the way daemons can be colocated within the same node. The resulting topology depends on the available hardware, as well as the amount of Ceph services present in the Controller nodes which are going to be retired. The following document describes the procedure required to migrate the RGW component (and keep an HA model using the Ceph Ingress daemon in a common TripleO scenario where Controller nodes represent the spec placement where the service is deployed. As a general rule, the number of services that can be migrated depends on the number of available nodes in the cluster. The following diagrams cover the distribution of the Ceph daemons on the CephStorage nodes where at least three nodes are required in a scenario that sees only RGW and RBD (no dashboard):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress <p>With dashboard, and without Manila at least four nodes are required (dashboard has no failover):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash dashboard/grafana osd rgw/ingress (free) <p>With dashboard and Manila 5 nodes minimum are required (and dashboard has no failover):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash mds/ganesha/ingress osd rgw/ingress mds/ganesha/ingress osd mds/ganesha/ingress dashboard/grafana"},{"location":"ceph/ceph_rgw/#current-status","title":"Current Status","text":"<pre><code>(undercloud) [stack@undercloud-0 ~]$ metalsmith list\n\n\n    +------------------------+    +----------------+\n    | IP Addresses           |    |  Hostname      |\n    +------------------------+    +----------------+\n    | ctlplane=192.168.24.25 |    | cephstorage-0  |\n    | ctlplane=192.168.24.10 |    | cephstorage-1  |\n    | ctlplane=192.168.24.32 |    | cephstorage-2  |\n    | ctlplane=192.168.24.28 |    | compute-0      |\n    | ctlplane=192.168.24.26 |    | compute-1      |\n    | ctlplane=192.168.24.43 |    | controller-0   |\n    | ctlplane=192.168.24.7  |    | controller-1   |\n    | ctlplane=192.168.24.41 |    | controller-2   |\n    +------------------------+    +----------------+\n</code></pre> <p>SSH into <code>controller-0</code> and check the <code>pacemaker</code> status: this will help identify the relevant information that we need to know before starting the RGW migration.</p> <pre><code>Full List of Resources:\n  * ip-192.168.24.46    (ocf:heartbeat:IPaddr2):        Started controller-0\n  * ip-10.0.0.103       (ocf:heartbeat:IPaddr2):        Started controller-1\n  * ip-172.17.1.129     (ocf:heartbeat:IPaddr2):        Started controller-2\n  * ip-172.17.3.68      (ocf:heartbeat:IPaddr2):        Started controller-0\n  * ip-172.17.4.37      (ocf:heartbeat:IPaddr2):        Started controller-1\n  * Container bundle set: haproxy-bundle\n\n[undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:\n    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-2\n    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-0\n    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-1\n</code></pre> <p>Use the <code>ip</code> command to identify the ranges of the storage networks.</p> <pre><code>[heat-admin@controller-0 ~]$ ip -o -4 a\n\n1: lo   inet 127.0.0.1/8 scope host lo\\     valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.45/24 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.46/32 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n7: br-ex    inet 10.0.0.122/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n8: vlan70   inet 172.17.5.22/24 brd 172.17.5.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n8: vlan70   inet 172.17.5.94/32 brd 172.17.5.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n9: vlan50   inet 172.17.2.140/24 brd 172.17.2.255 scope global vlan50\\      valid_lft forever preferred_lft forever\n10: vlan30  inet 172.17.3.73/24 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n10: vlan30  inet 172.17.3.68/32 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n11: vlan20  inet 172.17.1.88/24 brd 172.17.1.255 scope global vlan20\\       valid_lft forever preferred_lft forever\n12: vlan40  inet 172.17.4.24/24 brd 172.17.4.255 scope global vlan40\\       valid_lft forever preferred_lft forever\n</code></pre> <p>In this example:</p> <ul> <li>vlan30 represents the Storage Network, where the new RGW instances should be   started on the CephStorage nodes</li> <li>br-ex represents the External Network, which is where, in the current   environment, haproxy has the frontend VIP assigned</li> </ul>"},{"location":"ceph/ceph_rgw/#prerequisite-check-the-frontend-network-controller-nodes","title":"Prerequisite: check the frontend network (Controller nodes)","text":"<p>Identify the network that we previously had in haproxy and propagate it (via TripleO) to the CephStorage nodes. This network is used to reserve a new VIP that will be owned by Ceph and used as the entrypoint for the RGW service.</p> <p>ssh into <code>controller-0</code> and check the current HaProxy configuration until we find <code>ceph_rgw</code> section:</p> <pre><code>$ less /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg\n\n...\n...\nlisten ceph_rgw\n  bind 10.0.0.103:8080 transparent\n  bind 172.17.3.68:8080 transparent\n  mode http\n  balance leastconn\n  http-request set-header X-Forwarded-Proto https if { ssl_fc }\n  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }\n  http-request set-header X-Forwarded-Port %[dst_port]\n  option httpchk GET /swift/healthcheck\n  option httplog\n  option forwardfor\n  server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2\n  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2\n  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2\n</code></pre> <p>Double check the network used as HaProxy frontend:</p> <pre><code>[controller-0]$ ip -o -4 a\n\n...\n7: br-ex    inet 10.0.0.106/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n...\n</code></pre> <p>As described in the previous section, the check on controller-0 shows that we are exposing the services using the external network, which is not present in the CephStorage nodes, and we  need to propagate it via TripleO.</p>"},{"location":"ceph/ceph_rgw/#propagate-the-haproxy-frontend-network-to-cephstorage-nodes","title":"Propagate the <code>HaProxy</code> frontend network to <code>CephStorage</code> nodes","text":"<p>Change the nic template used to define the ceph-storage network interfaces and add the new config section.</p> <pre><code>---\nnetwork_config:\n- type: interface\n  name: nic1\n  use_dhcp: false\n  dns_servers: {{ ctlplane_dns_nameservers }}\n  addresses:\n  - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_subnet_cidr }}\n  routes: {{ ctlplane_host_routes }}\n- type: vlan\n  vlan_id: {{ storage_mgmt_vlan_id }}\n  device: nic1\n  addresses:\n  - ip_netmask: {{ storage_mgmt_ip }}/{{ storage_mgmt_cidr }}\n  routes: {{ storage_mgmt_host_routes }}\n- type: interface\n  name: nic2\n  use_dhcp: false\n  defroute: false\n- type: vlan\n  vlan_id: {{ storage_vlan_id }}\n  device: nic2\n  addresses:\n  - ip_netmask: {{ storage_ip }}/{{ storage_cidr }}\n  routes: {{ storage_host_routes }}\n- type: ovs_bridge\n  name: {{ neutron_physical_bridge_name }}\n  dns_servers: {{ ctlplane_dns_nameservers }}\n  domain: {{ dns_search_domains }}\n  use_dhcp: false\n  addresses:\n  - ip_netmask: {{ external_ip }}/{{ external_cidr }}\n  routes: {{ external_host_routes }}\n  members:\n  - type: interface\n    name: nic3\n    primary: true\n</code></pre> <p>In addition, add the External Network to the <code>baremetal.yaml</code> file used by metalsmith and run the <code>overcloud node provision</code> command passing the <code>--network-config</code> option:</p> <pre><code>- name: CephStorage\n  count: 3\n  hostname_format: cephstorage-%index%\n  instances:\n  - hostname: cephstorage-0\n  name: ceph-0\n  - hostname: cephstorage-1\n  name: ceph-1\n  - hostname: cephstorage-2\n  name: ceph-2\n  defaults:\n  profile: ceph-storage\n  network_config:\n      template: /home/stack/composable_roles/network/nic-configs/ceph-storage.j2\n  networks:\n  - network: ctlplane\n      vif: true\n  - network: storage\n  - network: storage_mgmt\n  - network: external\n</code></pre> <pre><code>(undercloud) [stack@undercloud-0]$\n\nopenstack overcloud node provision\n   -o overcloud-baremetal-deployed-0.yaml\n   --stack overcloud\n   --network-config -y\n  $PWD/network/baremetal_deployment.yaml\n</code></pre> <p>Check the new network on the <code>CephStorage</code> nodes:</p> <pre><code>[root@cephstorage-0 ~]# ip -o -4 a\n\n1: lo   inet 127.0.0.1/8 scope host lo\\     valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.54/24 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n11: vlan40  inet 172.17.4.43/24 brd 172.17.4.255 scope global vlan40\\       valid_lft forever preferred_lft forever\n12: vlan30  inet 172.17.3.23/24 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n14: br-ex   inet 10.0.0.133/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n</code></pre> <p>And now it\u2019s time to start migrating the RGW backends and build the ingress on top of them.</p>"},{"location":"ceph/ceph_rgw/#migrate-the-rgw-backends","title":"Migrate the RGW backends","text":"<p>To match the cardinality diagram we use cephadm labels to refer to a group of nodes where a given daemon type should be deployed.</p> <p>Add the RGW label to the cephstorage nodes:</p> <pre><code>for i in 0 1 2; {\n    ceph orch host label add cephstorage-$i rgw;\n}\n</code></pre> <pre><code>[ceph: root@controller-0 /]#\n\nfor i in 0 1 2; {\n    ceph orch host label add cephstorage-$i rgw;\n}\n\nAdded label rgw to host cephstorage-0\nAdded label rgw to host cephstorage-1\nAdded label rgw to host cephstorage-2\n\n[ceph: root@controller-0 /]# ceph orch host ls\n\nHOST        ADDR        LABELS          STATUS\ncephstorage-0  192.168.24.54  osd rgw\ncephstorage-1  192.168.24.44  osd rgw\ncephstorage-2  192.168.24.30  osd rgw\ncontroller-0   192.168.24.45  _admin mon mgr\ncontroller-1   192.168.24.11  _admin mon mgr\ncontroller-2   192.168.24.38  _admin mon mgr\n\n6 hosts in cluster\n</code></pre> <p>During the overcloud deployment, RGW is applied at step2 (external_deployment_steps), and a cephadm compatible spec is generated in <code>/home/ceph-admin/specs/rgw</code> from the ceph_mkspec ansible module. Find and patch the RGW spec, specifying the right placement using the labels approach and change the the rgw backend port to 8090 to avoid conflicts with the Ceph Ingress Daemon (*)</p> <pre><code>[root@controller-0 heat-admin]# cat rgw\n\nnetworks:\n- 172.17.3.0/24\nplacement:\n  hosts:\n  - controller-0\n  - controller-1\n  - controller-2\nservice_id: rgw\nservice_name: rgw.rgw\nservice_type: rgw\nspec:\n  rgw_frontend_port: 8080\n  rgw_realm: default\n  rgw_zone: default\n</code></pre> <p>Patch the spec replacing controller nodes with the label key</p> <pre><code>---\nnetworks:\n- 172.17.3.0/24\nplacement:\n  label: rgw\nservice_id: rgw\nservice_name: rgw.rgw\nservice_type: rgw\nspec:\n  rgw_frontend_port: 8090\n  rgw_realm: default\n  rgw_zone: default\n</code></pre> <p>(*) cephadm_check_port</p> <p>Apply the new RGW spec using the orchestrator CLI:</p> <pre><code>$ cephadm shell -m /home/ceph-admin/specs/rgw\n$ cephadm shell -- ceph orch apply -i /mnt/rgw\n</code></pre> <p>Which triggers the redeploy:</p> <pre><code>...\nosd.9                       cephstorage-2\nrgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090   starting\nrgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090   starting\nrgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090   starting\nrgw.rgw.controller-1.eyvrzw   controller-1   172.17.3.146:8080  running (5h)\nrgw.rgw.controller-2.navbxa   controller-2   172.17.3.66:8080   running (5h)\n\n...\nosd.9                       cephstorage-2\nrgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090  running (19s)\nrgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090  running (16s)\nrgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090  running (13s)\n</code></pre> <p>At this point, we need to make sure that the new RGW backends are reachable on the new ports, but we\u2019re going to enable an IngressDaemon on port 8080 later in the process. For this reason, ssh on each RGW node (the CephStorage nodes) and add the iptables rule to allow connections to both 8080 and 8090 ports in the CephStorage nodes.</p> <pre><code>iptables -I INPUT -p tcp -m tcp --dport 8080 -m conntrack --ctstate NEW -m comment --comment \"ceph rgw ingress\" -j ACCEPT\n\niptables -I INPUT -p tcp -m tcp --dport 8090 -m conntrack --ctstate NEW -m comment --comment \"ceph rgw backends\" -j ACCEPT\n\nfor port in 8080 8090; { \n    for i in 25 10 32; {\n       ssh heat-admin@192.168.24.$i sudo iptables -I INPUT \\\n       -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \\\n       -j ACCEPT;\n   }\n}\n</code></pre> <p>From a Controller node (e.g. controller-0) try to reach (curl) the rgw backends:</p> <pre><code>for i in 26 23 81; do {\n    echo \"----\"\n    curl 172.17.3.$i:8090;\n    echo \"----\"\n    echo\ndone\n</code></pre> <p>And you should observe the following:</p> <pre><code>----\nQuery 172.17.3.23\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n\n----\nQuery 172.17.3.26\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n\n----\nQuery 172.17.3.81\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n</code></pre>"},{"location":"ceph/ceph_rgw/#note","title":"NOTE","text":"<p>In case RGW backends are migrated in the CephStorage nodes, there\u2019s no \u201cinternalAPI\u201d network(this is not true in case of HCI). Reconfig the RGW keystone endpoint, pointing to the external Network that has been propagated (see previous section)</p> <pre><code>[ceph: root@controller-0 /]# ceph config dump | grep keystone\nglobal   basic rgw_keystone_url  http://172.16.1.111:5000\n\n[ceph: root@controller-0 /]# ceph config set global rgw_keystone_url http://10.0.0.103:5000\n</code></pre>"},{"location":"ceph/ceph_rgw/#deploy-a-ceph-ingressdaemon","title":"Deploy a Ceph IngressDaemon","text":"<p><code>HaProxy</code> is managed by TripleO via <code>Pacemaker</code>: the three running instances at this point will point to the old RGW backends, resulting in a wrong, not working configuration. Since we\u2019re going to deploy the Ceph Ingress Daemon, the first thing to do is remove the existing <code>ceph_rgw</code> config, cleanup the config created by TripleO and restart the service to make sure other services are not affected by this change.</p> <p>ssh  on each Controller node and remove the following is the section from <code>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg</code>:</p> <pre><code>listen ceph_rgw\n  bind 10.0.0.103:8080 transparent\n  mode http\n  balance leastconn\n  http-request set-header X-Forwarded-Proto https if { ssl_fc }\n  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }\n  http-request set-header X-Forwarded-Port %[dst_port]\n  option httpchk GET /swift/healthcheck\n  option httplog\n  option forwardfor\n   server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2\n  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2\n  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2\n</code></pre> <p>Restart <code>haproxy-bundle</code> and make sure it\u2019s started:</p> <pre><code>[root@controller-0 ~]# sudo pcs resource restart haproxy-bundle\nhaproxy-bundle successfully restarted\n\n\n[root@controller-0 ~]# sudo pcs status | grep haproxy\n\n  * Container bundle set: haproxy-bundle [undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:\n    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-0\n    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-1\n    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-2\n</code></pre> <p>Double check no process is bound to 8080 anymore\u201d</p> <pre><code>[root@controller-0 ~]# ss -antop | grep 8080\n[root@controller-0 ~]#\n</code></pre> <p>And the swift cli should fail at this point:</p> <pre><code>(overcloud) [root@cephstorage-0 ~]# swift list\n\nHTTPConnectionPool(host='10.0.0.103', port=8080): Max retries exceeded with url: /swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fc41beb0430&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n</code></pre> <p>Now we can start deploying the Ceph IngressDaemon on the CephStorage nodes.</p> <p>Set the required images for both HaProxy and Keepalived</p> <pre><code>[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_haproxy quay.io/ceph/haproxy:2.3\n\n[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_keepalived quay.io/ceph/keepalived:2.1.5\n</code></pre> <p>Prepare the ingress spec and mount it to cephadm:</p> <pre><code>$ sudo vim /home/ceph-admin/specs/rgw_ingress\n</code></pre> <p>and paste the following content:</p> <pre><code>---\nservice_type: ingress\nservice_id: rgw.rgw\nplacement:\n  label: rgw\nspec:\n  backend_service: rgw.rgw\n  virtual_ip: 10.0.0.89/24\n  frontend_port: 8080\n  monitor_port: 8898\n  virtual_interface_networks:\n    - 10.0.0.0/24\n</code></pre> <p>Mount the generated spec and apply it using the orchestrator CLI:</p> <pre><code>$ cephadm shell -m /home/ceph-admin/specs/rgw_ingress\n$ cephadm shell -- ceph orch apply -i /mnt/rgw_ingress\n</code></pre> <p>Wait until the ingress is deployed and query the resulting endpoint:</p> <pre><code>[ceph: root@controller-0 /]# ceph orch ls\n\nNAME                    PORTS               RUNNING  REFRESHED  AGE  PLACEMENT\ncrash                                           6/6  6m ago     3d   *\ningress.rgw.rgw         10.0.0.89:8080,8898     6/6  37s ago    60s  label:rgw\nmds.mds                   3/3  6m ago   3d   controller-0;controller-1;controller-2\nmgr                       3/3  6m ago   3d   controller-0;controller-1;controller-2\nmon                       3/3  6m ago   3d   controller-0;controller-1;controller-2\nosd.default_drive_group   15  37s ago   3d   cephstorage-0;cephstorage-1;cephstorage-2\nrgw.rgw   ?:8090          3/3  37s ago  4m   label:rgw\n</code></pre> <pre><code>[ceph: root@controller-0 /]# curl  10.0.0.89:8080\n\n---\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[ceph: root@controller-0 /]#\n\u2014\n</code></pre> <p>The result above shows that we\u2019re able to reach the backend from the IngressDaemon, which means we\u2019re almost ready to interact with it using the swift CLI.</p>"},{"location":"ceph/ceph_rgw/#update-the-object-store-endpoints","title":"Update the object-store endpoints","text":"<p>The endpoints still point to the old VIP owned by pacemaker, but given it\u2019s still used by other services and we reserved a new VIP on the same network, before any other action we should update the object-store endpoint.</p> <p>List the current endpoints:</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object\n\n| 1326241fb6b6494282a86768311f48d1 | regionOne | swift      | object-store   | True | internal  | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |\n| 8a34817a9d3443e2af55e108d63bb02b | regionOne | swift      | object-store   | True | public    | http://10.0.0.103:8080/swift/v1/AUTH_%(project_id)s  |\n| fa72f8b8b24e448a8d4d1caaeaa7ac58 | regionOne | swift      | object-store   | True | admin     | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |\n</code></pre> <p>Update the endpoints pointing to the Ingress VIP:</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint set --url \"http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s\" 95596a2d92c74c15b83325a11a4f07a3\n\n(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object-store\n| 6c7244cc8928448d88ebfad864fdd5ca | regionOne | swift      | object-store   | True | internal  | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |\n| 95596a2d92c74c15b83325a11a4f07a3 | regionOne | swift      | object-store   | True | public    | http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s   |\n| e6d0599c5bf24a0fb1ddf6ecac00de2d | regionOne | swift      | object-store   | True | admin     | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |\n</code></pre> <p>And repeat the same action for both internal and admin. Test the migrated service.</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ swift list --debug\n\nDEBUG:swiftclient:Versionless auth_url - using http://10.0.0.115:5000/v3 as endpoint\nDEBUG:keystoneclient.auth.identity.v3.base:Making authentication request to http://10.0.0.115:5000/v3/auth/tokens\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 10.0.0.115:5000\nDEBUG:urllib3.connectionpool:http://10.0.0.115:5000 \"POST /v3/auth/tokens HTTP/1.1\" 201 7795\nDEBUG:keystoneclient.auth.identity.v3.base:{\"token\": {\"methods\": [\"password\"], \"user\": {\"domain\": {\"id\": \"default\", \"name\": \"Default\"}, \"id\": \"6f87c7ffdddf463bbc633980cfd02bb3\", \"name\": \"admin\", \"password_expires_at\": null}, \n\n\n...\n...\n...\n\nDEBUG:swiftclient:REQ: curl -i http://10.0.0.89:8080/swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json -X GET -H \"X-Auth-Token: gAAAAABj7KHdjZ95syP4c8v5a2zfXckPwxFQZYg0pgWR42JnUs83CcKhYGY6PFNF5Cg5g2WuiYwMIXHm8xftyWf08zwTycJLLMeEwoxLkcByXPZr7kT92ApT-36wTfpi-zbYXd1tI5R00xtAzDjO3RH1kmeLXDgIQEVp0jMRAxoVH4zb-DVHUos\" -H \"Accept-Encoding: gzip\"\nDEBUG:swiftclient:RESP STATUS: 200 OK\nDEBUG:swiftclient:RESP HEADERS: {'content-length': '2', 'x-timestamp': '1676452317.72866', 'x-account-container-count': '0', 'x-account-object-count': '0', 'x-account-bytes-used': '0', 'x-account-bytes-used-actual': '0', 'x-account-storage-policy-default-placement-container-count': '0', 'x-account-storage-policy-default-placement-object-count': '0', 'x-account-storage-policy-default-placement-bytes-used': '0', 'x-account-storage-policy-default-placement-bytes-used-actual': '0', 'x-trans-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'x-openstack-request-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'accept-ranges': 'bytes', 'content-type': 'application/json; charset=utf-8', 'date': 'Wed, 15 Feb 2023 09:11:57 GMT'}\nDEBUG:swiftclient:RESP BODY: b'[]'\n</code></pre> <p>Run tempest tests against object-storage:</p> <pre><code>(overcloud) [stack@undercloud-0 tempest-dir]$  tempest run --regex tempest.api.object_storage\n...\n...\n...\n======\nTotals\n======\nRan: 141 tests in 606.5579 sec.\n - Passed: 128\n - Skipped: 13\n - Expected Fail: 0\n - Unexpected Success: 0\n - Failed: 0\nSum of execute time for each test: 657.5183 sec.\n\n==============\nWorker Balance\n==============\n - Worker 0 (1 tests) =&gt; 0:10:03.400561\n - Worker 1 (2 tests) =&gt; 0:00:24.531916\n - Worker 2 (4 tests) =&gt; 0:00:10.249889\n - Worker 3 (30 tests) =&gt; 0:00:32.730095\n - Worker 4 (51 tests) =&gt; 0:00:26.246044\n - Worker 5 (6 tests) =&gt; 0:00:20.114803\n - Worker 6 (20 tests) =&gt; 0:00:16.290323\n - Worker 7 (27 tests) =&gt; 0:00:17.103827\n</code></pre>"},{"location":"ceph/ceph_rgw/#additional-resources","title":"Additional Resources","text":"<p>A screen recording is available here.</p>"},{"location":"contributing/documentation/","title":"Contributing to documentation","text":""},{"location":"contributing/documentation/#rendering-documentation-locally","title":"Rendering documentation locally","text":"<p>Install docs build requirements into virtualenv:</p> <pre><code>python3 -m venv local/docs-venv\nsource local/docs-venv/bin/activate\npip install -r docs/doc_requirements.txt\n</code></pre> <p>Serve docs site on localhost:</p> <pre><code>mkdocs serve\n</code></pre> <p>Click the link it outputs. As you save changes to files modified in your editor, the browser will automatically show the new content.</p>"},{"location":"contributing/documentation/#patterns-and-tips-for-contributing-to-documentation","title":"Patterns and tips for contributing to documentation","text":"<ul> <li> <p>Pages concerning individual components/services should make sense in   the context of the broader adoption procedure. While adopting a   service in isolation is an option for developers, let's write the   documentation with the assumption the adoption procedure is being   done in full, going step by step (one doc after another).</p> </li> <li> <p>The procedure should be written with production use in mind. This   repository could be used as a starting point for product   technical documentation. We should not tie the documentation to   something that wouldn't translate well from dev envs to production.</p> <ul> <li>This includes not assuming that the source environment is   Standalone, and destination is CRC. We can provide examples for   Standalone/CRC, but it should be possible to use the procedure   with fuller environments in a way that is obvious from the docs.</li> </ul> </li> <li> <p>If possible, try to make code snippets copy-pastable. Use shell   variables if the snippets should be parametrized. Use <code>oc</code> rather   than <code>kubectl</code> in snippets.</p> </li> <li> <p>Focus on the \"happy path\" in the docs as much as possible,   troubleshooting info can go into the Troubleshooting page, or   alternatively a troubleshooting section at the end of the document,   visibly separated from the main procedure.</p> </li> <li> <p>The full procedure will inevitably happen to be quite long, so let's   try to be concise in writing to keep the docs consumable (but not to   a point of making things difficult to understand or omitting   important things).</p> </li> </ul>"},{"location":"contributing/tests/","title":"Tests","text":""},{"location":"contributing/tests/#test-suite-information","title":"Test suite information","text":"<p>The adoption docs repository also includes a test suite for Adoption. Currently only one test target is defined:</p> <ul> <li><code>minimal</code> - a minimal test scenario, the eventual set of services in   this scenario should be the \"core\" services needed to launch a VM   (without Ceph, to keep environment size requirements small and make   it easy to set up).</li> </ul> <p>We can add more scenarios as we go (e.g. one that includes Ceph).</p>"},{"location":"contributing/tests/#running-the-tests","title":"Running the tests","text":"<p>The interface between the execution infrastructure and the test suite is an Ansible inventory and variables files. Inventory and variable samples are provided. To run the tests, follow this procedure:</p> <ul> <li> <p>Create <code>tests/inventory.yaml</code> file by copying and editing one of the   included samples (e.g. <code>tests/inventory.sample-crc-vagrant.yaml</code>) to   provide values valid in your environment.</p> </li> <li> <p>Create <code>tests/vars.yaml</code> and <code>tests/secrets.yaml</code>, likewise by   copying and editing the included samples (<code>tests/vars.sample.yaml</code>,   <code>tests/secrets.sample.yaml</code>).</p> </li> <li> <p>Run <code>make test-minimal</code>.</p> </li> </ul>"},{"location":"openstack/backend_services_deployment/","title":"Backend services deployment","text":"<p>The following instructions create OpenStackControlPlane CR with MariaDB and RabbitMQ deployed, and other services disabled. This will be the foundation of the podified control plane.</p> <p>In subsequent steps we'll import the original databases and then add podified OpenStack control plane services.</p>"},{"location":"openstack/backend_services_deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The cloud which we want to adopt is up and running. It's on   OpenStack Wallaby release.</p> </li> <li> <p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is   not deployed.</p> </li> </ul> <p>For developer/CI environments, the openstack operator can be deployed   by running <code>make openstack</code> inside   install_yamls   repo.</p> <p>For production environments, the deployment method will likely be   different.</p> <ul> <li>There are free PVs available to be claimed (for MariaDB and RabbitMQ).</li> </ul> <p>For developer/CI environments driven by install_yamls, make sure   you've run <code>make crc_storage</code>.</p>"},{"location":"openstack/backend_services_deployment/#variables","title":"Variables","text":"<ul> <li>Set the desired admin password for the podified deployment. This can   be the original deployment's admin password or something else.</li> </ul> <pre><code>ADMIN_PASSWORD=SomePassword\n</code></pre> <ul> <li>Set service password variables to match the original deployment.   Database passwords can differ in podified environment, but   synchronizing the service account passwords is a required step.</li> </ul> <p>E.g. in developer environments with TripleO Standalone, the   passwords can be extracted like this:</p> <pre><code>CINDER_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')\nGLANCE_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')\nIRONIC_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')\nNEUTRON_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')\nNOVA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')\nOCTAVIA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')\nPLACEMENT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')\n</code></pre>"},{"location":"openstack/backend_services_deployment/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/backend_services_deployment/#procedure-backend-services-deployment","title":"Procedure - backend services deployment","text":"<ul> <li>Create OSP secret.</li> </ul> <p>The procedure for this will vary, but in developer/CI environments   we use install_yamls:</p> <pre><code># in install_yamls\nmake input\n</code></pre> <ul> <li>If the <code>$ADMIN_PASSWORD</code> is different than the already set password   in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>   correspondingly:</li> </ul> <pre><code>oc set data secret/osp-secret \"AdminPassword=$ADMIN_PASSWORD\"\n</code></pre> <ul> <li>Set service account passwords in <code>osp-secret</code> to match the service   account passwords from original deployment:</li> </ul> <pre><code>oc set data secret/osp-secret \"CinderPassword=$CINDER_PASSWORD\"\noc set data secret/osp-secret \"GlancePassword=$GLANCE_PASSWORD\"\noc set data secret/osp-secret \"IronicPassword=$IRONIC_PASSWORD\"\noc set data secret/osp-secret \"NeutronPassword=$NEUTRON_PASSWORD\"\noc set data secret/osp-secret \"NovaPassword=$NOVA_PASSWORD\"\noc set data secret/osp-secret \"OctaviaPassword=$OCTAVIA_PASSWORD\"\noc set data secret/osp-secret \"PlacementPassword=$PLACEMENT_PASSWORD\"\n</code></pre> <ul> <li>Deploy OpenStackControlPlane. Make sure to only enable MariaDB and   RabbitMQ services. All other services must be disabled.</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: core.openstack.org/v1beta1\nkind: OpenStackControlPlane\nmetadata:\n  name: openstack\nspec:\n  secret: osp-secret\n  storageClass: local-storage\n  mariadb:\n    template:\n      containerImage: quay.io/tripleozedcentos9/openstack-mariadb:current-tripleo\n      storageRequest: 500M\n  rabbitmq:\n    template:\n      replicas: 1\n\n  keystone:\n    enabled: false\n  cinder:\n    enabled: false\n  glance:\n    enabled: false\n  placement:\n    enabled: false\n  ovn:\n    enabled: false\n  ovs:\n    enabled: false\n  neutron:\n    enabled: false\n  nova:\n    enabled: false\nEOF\n</code></pre>"},{"location":"openstack/backend_services_deployment/#post-checks","title":"Post-checks","text":"<ul> <li>Check that MariaDB is running.</li> </ul> <pre><code>oc get pod mariadb-openstack -o jsonpath='{.status.phase}{\"\\n\"}'\n</code></pre>"},{"location":"openstack/glance_adoption/","title":"Glance adoption","text":"<p>Adopting Glance means that an existing <code>OpenStackControlPlane</code> CR, where Glance is supposed to be disabled, should be patched to start the service with the configuration parameters provided by the source environment.</p> <p>When the procedure is over, the expectation is to see the <code>GlanceAPI</code> service up and running: the <code>Keystone endpoints</code> should be updated and the same backend of the source Cloud will be available. If the conditions above are met, the adoption is considered concluded.</p> <p>This guide also assumes that:</p> <ol> <li>A <code>TripleO</code> environment (the source Cloud) is running on one side;</li> <li>A <code>SNO</code> / <code>CodeReadyContainers</code> is running on the other side;</li> <li>(optional) an internal/external <code>Ceph</code> cluster is reacheable by both <code>crc</code> and <code>TripleO</code></li> </ol>"},{"location":"openstack/glance_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, MariaDB and Keystone   should be already adopted.</li> </ul>"},{"location":"openstack/glance_adoption/#pre-check","title":"Pre-check","text":"<p>On the source Cloud, check that the service is active and works as expected, and list the existing images:</p> <pre><code>(openstack)$ image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n</code></pre>"},{"location":"openstack/glance_adoption/#procedure-glance-adoption","title":"Procedure - Glance adoption","text":"<p>As already done for Keystone, the Glance Adoption follows the same pattern.</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      glanceAPIExternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n'\n</code></pre> <p>However, if a Ceph backend is used, the <code>customServiceConfig</code> parameter should be used to inject the right configuration to the <code>GlanceAPI</code> instance.</p> <p>Make sure the Ceph related secret exists in the <code>openstack</code> namespace:</p> <pre><code>$ oc get secrets | grep ceph\nceph-conf-files\n</code></pre> <p>If it doesn't exist, create a <code>Secret</code> which contains the <code>Cephx</code> key and Ceph configuration file so that the Glance pod created by the operator can mount those files in <code>/etc/ceph</code>.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ceph-client-conf\n  namespace: openstack\nstringData:\n  ceph.client.openstack.keyring: |\n    [client.openstack]\n        key = &lt;secret key&gt;\n        caps mgr = \"allow *\"\n        caps mon = \"profile rbd\"\n        caps osd = \"profile rbd pool=images\"\n  ceph.conf: |\n    [global]\n    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4\n    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4\n</code></pre> <p>This secret will be used in the <code>extraVolumes</code> parameters to propagate the files to the <code>GlanceAPI</code> pods (both internal and external).</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      customServiceConfig: |\n        [DEFAULT]\n        enabled_backends=default_backend:rbd\n        [glance_store]\n        default_backend=default_backend\n        [default_backend]\n        rbd_store_ceph_conf=/etc/ceph/ceph.conf\n        rbd_store_user=openstack\n        rbd_store_pool=images\n        store_description=Ceph glance store backend.\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      glanceAPIExternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n  extraMounts:\n    - extraVol:\n      - propagation:\n        - Glance\n        extraVolType: Ceph\n        volumes:\n        - name: ceph\n          projected:\n            sources:\n            - secret:\n                name: ceph-conf-files\n        mounts:\n        - name: ceph\n          mountPath: \"/etc/ceph\"\n          readOnly: true\n'\n</code></pre>"},{"location":"openstack/glance_adoption/#post-checks","title":"Post-checks","text":""},{"location":"openstack/glance_adoption/#test-the-glance-service-from-the-openstack-cli","title":"Test the glance service from the OpenStack cli","text":"<p>Inspect the resulting glance pods:</p> <pre><code>sh-5.1# cat /etc/glance/glance.conf.d/01-custom.conf\n\n[DEFAULT]\nenabled_backends=default_backend:rbd\n[glance_store]\ndefault_backend=default_backend\n[default_backend]\nrbd_store_ceph_conf=/etc/ceph/ceph.conf\nrbd_store_user=openstack\nrbd_store_pool=images\nstore_description=Ceph glance store backend.\n\nsh-5.1# ls /etc/ceph/ceph*\n/etc/ceph/ceph.client.openstack.keyring  /etc/ceph/ceph.conf\n</code></pre> <p>Ceph secrets are properly mounted, at this point let's move to the openstack cli and check the service is active and the endpoints are properly updated.</p> <pre><code>(openstack)$ service list | grep image\n\n| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |\n\n(openstack)$ endpoint list | grep image\n\n| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |\n| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |\n| 709859219bc24ab9ac548eab74ad4dd5 | regionOne | glance       | image        | True    | admin     | http://glance-admin-openstack.apps-crc.testing      |\n</code></pre> <p>Check the images that we previously listed in the source Cloud are available in the adopted service:</p> <pre><code>(openstack)$ image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n</code></pre>"},{"location":"openstack/glance_adoption/#image-upload","title":"Image upload","text":"<p>We can test that an image can be created on from the adopted service.</p> <pre><code>(openstack)$ export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\n(openstack)$ export OS_CLOUD=adopted\n(openstack)$ curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img\n    qemu-img convert -O raw /tmp/cirros-0.5.2-x86_64-disk.img /tmp/cirros-0.5.2-x86_64-disk.img.raw\n    openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img.raw cirros2\n    openstack image list\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   273  100   273    0     0   1525      0 --:--:-- --:--:-- --:--:--  1533\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 15.5M  100 15.5M    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M\n\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| Field            | Value                                                                                                                                      |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| container_format | bare                                                                                                                                       |\n| created_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| disk_format      | raw                                                                                                                                        |\n| file             | /v2/images/46a3eac1-7224-40bc-9083-f2f0cd122ba4/file                                                                                       |\n| id               | 46a3eac1-7224-40bc-9083-f2f0cd122ba4                                                                                                       |\n| min_disk         | 0                                                                                                                                          |\n| min_ram          | 0                                                                                                                                          |\n| name             | cirros                                                                                                                                     |\n| owner            | 9f7e8fdc50f34b658cfaee9c48e5e12d                                                                                                           |\n| properties       | os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/cirros', owner_specified.openstack.sha256='' |\n| protected        | False                                                                                                                                      |\n| schema           | /v2/schemas/image                                                                                                                          |\n| status           | queued                                                                                                                                     |\n| tags             |                                                                                                                                            |\n| updated_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| visibility       | shared                                                                                                                                     |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| 46a3eac1-7224-40bc-9083-f2f0cd122ba4 | cirros2| active |\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n\n\n(openstack)$ oc rsh ceph\nsh-4.4$ ceph -s\nr  cluster:\n    id:     432d9a34-9cee-4109-b705-0c59e8973983\n    health: HEALTH_OK\n\n  services:\n    mon: 1 daemons, quorum a (age 4h)\n    mgr: a(active, since 4h)\n    osd: 1 osds: 1 up (since 4h), 1 in (since 4h)\n\n  data:\n    pools:   5 pools, 160 pgs\n    objects: 46 objects, 224 MiB\n    usage:   247 MiB used, 6.8 GiB / 7.0 GiB avail\n    pgs:     160 active+clean\n\nsh-4.4$ rbd -p images ls\n46a3eac1-7224-40bc-9083-f2f0cd122ba4\nc3158cad-d50b-452f-bec1-f250562f5c1f\n</code></pre>"},{"location":"openstack/keystone_adoption/","title":"Keystone adoption","text":""},{"location":"openstack/keystone_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, the service databases   must already be imported into the podified MariaDB.</li> </ul>"},{"location":"openstack/keystone_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/keystone_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/keystone_adoption/#procedure-keystone-adoption","title":"Procedure - Keystone adoption","text":"<ul> <li>Patch OpenStackControlPlane to deploy Keystone:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  keystone:\n    enabled: true\n    template:\n      secret: osp-secret\n      containerImage: quay.io/tripleozedcentos9/openstack-keystone:current-tripleo\n      databaseInstance: openstack\n'\n</code></pre> <ul> <li>Create a clouds.yaml file to talk to adopted Keystone:</li> </ul> <pre><code>cat &gt; clouds-adopted.yaml &lt;&lt;EOF\nclouds:\n  adopted:\n    auth:\n      auth_url: http://keystone-public-openstack.apps-crc.testing\n      password: $ADMIN_PASSWORD\n      project_domain_name: Default\n      project_name: admin\n      user_domain_name: Default\n      username: admin\n    cacert: ''\n    identity_api_version: '3'\n    region_name: regionOne\n    volume_api_version: '3'\nEOF\n</code></pre> <ul> <li>Clean up old endpoints that still point to old control plane   (everything except Keystone endpoints):</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep ' cinderv3 ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' glance ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' neutron ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' nova ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' placement ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' swift ' | awk '{ print $2; }' | xargs openstack endpoint delete\n</code></pre>"},{"location":"openstack/keystone_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that Keystone endpoints are defined and pointing to the podified   FQDNs:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep keystone\n</code></pre>"},{"location":"openstack/mariadb_copy/","title":"MariaDB data copy","text":"<p>This document describes how to move the databases from the original OpenStack deployment to the MariaDB instances in the OpenShift cluster.</p>"},{"location":"openstack/mariadb_copy/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Make sure the previous Adoption steps have been performed successfully.</p> </li> <li> <p>The OpenStackControlPlane resource must be already created at this point.</p> </li> <li> <p>Podified MariaDB and RabbitMQ are running. No other podified     control plane services are running.</p> </li> <li> <p>There must be network routability between:</p> <ul> <li> <p>The adoption host and the original MariaDB.</p> </li> <li> <p>The adoption host and the podified MariaDB.</p> </li> <li> <p>Note that this routability requirement may change in the   future, e.g. we may require routability from original MariaDB to   podified MariaDB.</p> </li> </ul> </li> </ul>"},{"location":"openstack/mariadb_copy/#variables","title":"Variables","text":"<p>Define the shell variables used in the steps below. The values are just illustrative, use values which are correct for your environment:</p> <pre><code>PODIFIED_MARIADB_IP=$(oc get -o yaml pod mariadb-openstack | grep podIP: | awk '{ print $2; }')\nMARIADB_IMAGE=quay.io/tripleozedcentos9/openstack-mariadb:current-tripleo\n\n# Use your environment's values for these:\nEXTERNAL_MARIADB_IP=192.168.24.3\nEXTERNAL_DB_ROOT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')\nPODIFIED_DB_ROOT_PASSWORD=12345678\n</code></pre>"},{"location":"openstack/mariadb_copy/#pre-checks","title":"Pre-checks","text":"<ul> <li>Test connection to the original DB (show databases):</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysql -h \"$EXTERNAL_MARIADB_IP\" -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre> <ul> <li>Run mysqlcheck on the original DB:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysqlcheck --all-databases -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\"\n</code></pre> <ul> <li>Test connection to podified DB (show databases):</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n    mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"openstack/mariadb_copy/#procedure-stopping-control-plane-services","title":"Procedure - stopping control plane services","text":"<p>From each controller node it is necessary to stop the control-plane services to avoid inconsistencies in the data migrated for the data-plane adoption procedure.</p> <p>1- Connect to all the controller nodes and stop the control plane services.</p> <p>2- Stop the services.</p> <pre><code># Configure SSH variables to stop the services\n# in each controller node. For example:\n\nCONTROLLER1_SSH=\"ssh -F $ENV_DIR/director_standalone/vagrant_ssh_config vagrant@standalone\"\nCONTROLLER2_SSH=\":\"\nCONTROLLER3_SSH=\":\"\n\n# Update the services list to be stoped\n\nServicesToStop=(\"tripleo_horizon.service\"\n\"tripleo_keystone.service\"\n\"tripleo_cinder_api.service\"\n\"tripleo_glance_api.service\"\n\"tripleo_neutron_api.service\"\n\"tripleo_nova_api.service\"\n\"tripleo_placement_api.service\")\n\necho \"Stopping the OpenStack services\"\n\nfor service in ${ServicesToStop[*]}; do\necho \"Stopping the service: $service in each controller node\"\n$CONTROLLER1_SSH sudo systemctl stop $service\n$CONTROLLER2_SSH sudo systemctl stop $service\n$CONTROLLER3_SSH sudo systemctl stop $service\ndone\n</code></pre> <p>3- Make sure all the services are stopped</p>"},{"location":"openstack/mariadb_copy/#procedure-data-copy","title":"Procedure - data copy","text":"<ul> <li>Create a temporary folder to store DB dumps and make sure it's the   working directory for the following steps:</li> </ul> <pre><code>mkdir ~/adoption-db\ncd ~/adoption-db\n</code></pre> <ul> <li>Create a dump of the original databases:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE bash &lt;&lt;EOF\n\nmysql -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -N -e 'show databases' | while read dbname; do\n    echo \"Dumping \\$dbname\"\n    mysqldump -h $EXTERNAL_MARIADB_IP -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" \\\n        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \\\n        --databases \"\\$dbname\" \\\n        &gt; \"\\$dbname\".sql\ndone\n\nEOF\n</code></pre> <ul> <li>Restore the databases from .sql files into the podified MariaDB:</li> </ul> <pre><code>for dbname in cinder glance keystone nova_api nova_cell0 nova ovs_neutron placement; do\n    echo \"Restoring $dbname\"\n    oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n       mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" &lt; \"$dbname.sql\"\ndone\n</code></pre>"},{"location":"openstack/mariadb_copy/#post-checks","title":"Post-checks","text":"<ul> <li>Check that the databases were imported correctly:</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n   mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"openstack/other_services_adoption/","title":"Adoption of other services","text":"<p>This part of the guide adopts the remaining services that don't have a specific guide of their own. It is likely that as adoption gets developed further, services will be removed from here and put into their own guides (e.g. like Glance).</p>"},{"location":"openstack/other_services_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed.</li> </ul>"},{"location":"openstack/other_services_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/other_services_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/other_services_adoption/#procedure-adoption-of-other-services","title":"Procedure - Adoption of other services","text":"<ul> <li>Deploy the rest of control plane services:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  # cinder:\n  #   enabled: true\n  #   template:\n  #     cinderAPI:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-api:current-tripleo\n  #     cinderScheduler:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-scheduler:current-tripleo\n  #     cinderBackup:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-backup:current-tripleo\n  #     cinderVolumes:\n  #       volume1:\n  #         containerImage: quay.io/tripleozedcentos9/openstack-cinder-volume:current-tripleo\n  #         replicas: 1\n\n  placement:\n    enabled: true\n    template:\n      containerImage: quay.io/tripleozedcentos9/openstack-placement-api:current-tripleo\n      databaseInstance: openstack\n      secret: osp-secret\n\n  ovn:\n    enabled: true\n    template:\n      ovnDBCluster:\n        ovndbcluster-nb:\n          replicas: 1\n          containerImage: quay.io/tripleozedcentos9/openstack-ovn-nb-db-server:current-tripleo\n          dbType: NB\n          storageRequest: 10G\n        ovndbcluster-sb:\n          replicas: 1\n          containerImage: quay.io/tripleozedcentos9/openstack-ovn-sb-db-server:current-tripleo\n          dbType: SB\n          storageRequest: 10G\n      ovnNorthd:\n        replicas: 1\n        containerImage: quay.io/tripleozedcentos9/openstack-ovn-northd:current-tripleo\n\n  ovs:\n    enabled: true\n    template:\n      ovsContainerImage: \"quay.io/skaplons/ovs:latest\"\n      ovnContainerImage: \"quay.io/tripleozedcentos9/openstack-ovn-controller:current-tripleo\"\n      external-ids:\n        system-id: \"random\"\n        ovn-bridge: \"br-int\"\n        ovn-encap-type: \"geneve\"\n\n  neutron:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-neutron-server:current-tripleo\n      secret: osp-secret\n\n  # nova:\n  #   enabled: true\n  #   template:\n  #     secret: osp-secret\n'\n</code></pre>"},{"location":"openstack/other_services_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that service endpoints are defined:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list\n</code></pre>"},{"location":"openstack/troubleshooting/","title":"Troubleshooting","text":"<p>This document contains information about various issues you might face and how to solve them.</p>"},{"location":"openstack/troubleshooting/#errimagepull-due-to-missing-authentication","title":"ErrImagePull due to missing authentication","text":"<p>The deployed containers pull the images from private containers registries that can potentially return authentication errors like: <code>Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\": rpc error: code = Unknown desc = unable to retrieve auth token: invalid username/password: unauthorized: Please login to the Red Hat Registry using your Customer Portal credentials.</code></p> <p>An example of a failed pod:</p> <pre><code>  Normal   Scheduled       3m40s                  default-scheduler  Successfully assigned openstack/rabbitmq-server-0 to worker0\n  Normal   AddedInterface  3m38s                  multus             Add eth0 [10.101.0.41/23] from ovn-kubernetes\n  Warning  Failed          2m16s (x6 over 3m38s)  kubelet            Error: ImagePullBackOff\n  Normal   Pulling         2m5s (x4 over 3m38s)   kubelet            Pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\": rpc error: code  ... can be found here: https://access.redhat.com/RegistryAuthentication\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Error: ErrImagePull\n  Normal   BackOff         110s (x7 over 3m38s)   kubelet            Back-off pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n</code></pre> <p>In order to solve this issue we need to get a valid pull-secret from the official Red Hat console site, store this pull secret locally in a machine with access to the Kubernetes API (service node), and then run:</p> <pre><code>oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location.json&gt;\n</code></pre> <p>The previous commmand will make available the authentication information in all the cluster's compute nodes, then trigger a new pod deployment to pull the container image with:</p> <pre><code>kubectl delete pod rabbitmq-server-0 -n openstack\n</code></pre> <p>And the pod should be able to pull the image successfully. For more inforation about what container registries requires what type of authentication, check the official docs.</p>"}]}