{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Plane Adoption procedure","text":""},{"location":"#openstack-adoption","title":"OpenStack adoption","text":"<p>This is a procedure for adopting an OpenStack cloud.</p> <p>Perform the actions from the sub-documents in the following order:</p> <ul> <li> <p>Deploy podified backend services</p> </li> <li> <p>Copy MariaDB data</p> </li> <li> <p>OVN adoption</p> </li> <li> <p>Keystone adoption</p> </li> <li> <p>Glance adoption</p> </li> <li> <p>Placement adoption</p> </li> <li> <p>Adoption of other services</p> </li> </ul> <p>If you face issues during adoption, check the Troubleshooting document for common problems and solutions.</p>"},{"location":"#post-openstack-ceph-adoption","title":"Post-OpenStack Ceph adoption","text":"<p>If the environment includes Ceph and some of its services are collocated on the Controller hosts (\"internal Ceph\"), then Ceph services need to be moved out of Controller hosts as the last step of the OpenStack adoption. Follow this documentation:</p> <ul> <li>Ceph RBD migration</li> <li>Ceph RGW migration</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>For information about contributing to the docs and how to run tests, see:</p> <ul> <li> <p>Contributing to documentation -   how to build docs locally, docs patterns and tips.</p> </li> <li> <p>Tests -   information about the test suite and how to run it.</p> </li> </ul>"},{"location":"ceph/ceph_rbd/","title":"Data Plane adoption - Ceph RBD Migration","text":"<p>In this scenario, assuming Ceph is already &gt;= 5, either for HCI or dedicated Storage nodes, the daemons living in the OpenStack control plane should be moved/migrated into the existing external RHEL nodes (typically the compute nodes for an HCI environment or dedicated storage nodes in all the remaining use cases).</p>"},{"location":"ceph/ceph_rbd/#requirements","title":"Requirements","text":"<ul> <li>Ceph is &gt;= 5 and managed by cephadm/orchestrator.</li> <li>Ceph NFS (ganesha) migrated from a TripleO based deployment to cephadm.</li> <li>Both the Ceph public and cluster networks are propagated, via TripleO, to the target nodes.</li> <li>Ceph Mons need to keep their IPs (to avoid cold migration).</li> </ul>"},{"location":"ceph/ceph_rbd/#scenario-1-migrate-mon-and-mgr-from-controller-nodes","title":"SCENARIO 1: Migrate mon and mgr from controller nodes","text":"<p>The goal of the first POC is to prove we are able to successfully drain a controller node, in terms of ceph daemons, and move them to a different node. The initial target of the POC is RBD only, which means we\u2019re going to move only mon and mgr daemons. For the purposes of this POC, we'll deploy a ceph cluster with only mon, mgrs, and osds to simulate the environment a customer will be in before starting the migration. The goal of the first POC is to ensure that: - We can keep the mon IP addresses moving them to the CephStorage nodes. - We can drain the existing controller nodes and shut them down. - We can deploy additional monitors to the existing nodes, promoting them as   _admin nodes that can be used by administrators to manage the ceph cluster   and perform day2 operations against it. - We can keep the cluster operational during the migration.</p>"},{"location":"ceph/ceph_rbd/#prerequisites","title":"Prerequisites","text":"<p>The Storage Nodes should be configured to have both storage and storage_mgmt network to make sure we can use both Ceph public and cluster networks.</p> <p>This step is the only one where the interaction with TripleO is required. From 17+ we don\u2019t have to run any stack update, however, we have commands that should be performed to run os-net-config on the bare-metal node and configure additional networks.</p> <p>Make sure the network is defined in metalsmith.yaml for the CephStorageNodes:</p> <pre><code>- name: CephStorage\n  count: 2\n  instances:\n    - hostname: oc0-ceph-0\n      name: oc0-ceph-0\n    - hostname: oc0-ceph-1\n      name: oc0-ceph-1\n  defaults:\n    networks:\n      - network: ctlplane\n        vif: true\n      - network: storage_cloud_0\n          subnet: storage_cloud_0_subnet\n      - network: storage_mgmt_cloud_0\n          subnet: storage_mgmt_cloud_0_subnet\n    network_config:\n      template: templates/single_nic_vlans/single_nic_vlans_storage.j2\n</code></pre> <p>Then run:</p> <pre><code>openstack overcloud node provision \\\n  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \\\n  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam\n</code></pre> <p>Verify that the storage network is running on the node:</p> <pre><code>(undercloud) [CentOS-9 - stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a\nWarning: Permanently added '192.168.24.14' (ED25519) to the list of known hosts.\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"ceph/ceph_rbd/#migrate-mons-and-mgrs-on-the-two-existing-cephstorage-nodes","title":"Migrate mon(s) and mgr(s) on the two existing CephStorage nodes","text":"<p>Create a ceph spec based on the default roles with the mon/mgr on the controller nodes.</p> <pre><code>openstack overcloud ceph spec -o ceph_spec.yaml -y  \\\n   --stack overcloud-0     overcloud-baremetal-deployed-0.yaml\n</code></pre> <p>Deploy the Ceph cluster</p> <pre><code> openstack overcloud ceph deploy overcloud-baremetal-deployed-0.yaml \\\n    --stack overcloud-0 -o deployed_ceph.yaml \\\n    --network-data ~/oc0-network-data.yaml \\\n    --ceph-spec ~/ceph_spec.yaml\n</code></pre> <p>Note:</p> <p>The ceph_spec.yaml, which is the OSP-generated description of the ceph cluster, will be used, later in the process, as the basic template required by cephadm to update the status/info of the daemons.</p> <p>Check the status of the cluster:</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2 (age 19m)\n    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk\n    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  _admin mgr mon\noc0-controller-1  192.168.24.23  _admin mgr mon\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>The goal of the next section is to migrate the oc0-controller-{1,2} daemons into oc0-ceph-{0,1} as the very basic scenario that demonstrates we can actually make this kind of migration using cephadm.</p>"},{"location":"ceph/ceph_rbd/#migrate-oc0-controller-1-into-oc0-ceph-0","title":"Migrate oc0-controller-1 into oc0-ceph-0","text":"<p>ssh into controller-0, then</p> <p><code>cephadm shell -v /home/ceph-admin/specs:/specs</code></p> <p>ssh into ceph-0, then</p> <p><code>sudo \u201cwatch podman ps\u201d  # watch the new mon/mgr being deployed here</code></p> <p>(optional) if mgr is active in the source node, then:</p> <pre><code>ceph mgr fail &lt;mgr instance&gt;\n</code></pre> <p>From the cephadm shell, remove the labels on oc0-controller-1</p> <pre><code>    for label in mon mgr _admin; do\n           ceph orch host rm label oc0-controller-1 $label;\n    done\n</code></pre> <p>Add the missing labels to oc0-ceph-0</p> <pre><code>[ceph: root@oc0-controller-0 /]#\n&gt; for label in mon mgr _admin; do ceph orch host label add oc0-ceph-0 $label; done\nAdded label mon to host oc0-ceph-0\nAdded label mgr to host oc0-ceph-0\nAdded label _admin to host oc0-ceph-0\n</code></pre> <p>Drain and force-remove the oc0-controller-1 node</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host drain oc0-controller-1\nScheduled to remove the following daemons from host 'oc0-controller-1'\ntype                 id\n-------------------- ---------------\nmon                  oc0-controller-1\nmgr                  oc0-controller-1.mtxohd\ncrash                oc0-controller-1\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host rm oc0-controller-1 --force\nRemoved  host 'oc0-controller-1'\n\n[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  mgr mon _admin\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>If you have only 3 mon nodes, and the drain of the node doesn\u2019t work as expected (the containers are still there), then SSH to controller-1 and force-purge the containers in the node:</p> <pre><code>[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES\n5c1ad36472bc  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1\n3b14cc7bf4dd  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd\n\n[root@oc0-controller-1 ~]# cephadm rm-cluster --fsid f6ec3ebe-26f7-56c8-985d-eb974e8e08e3 --force\n\n[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES\n</code></pre> <p>Note: cephadm rm-cluster on a node that is not part of the cluster anymore has the effect of removing all the containers and doing some cleanup on the filesystem.</p> <p>Before shutting the oc0-controller-1 down, move the IP address (on the same network) to the oc0-ceph-0 node:</p> <pre><code>mon_host = [v2:172.16.11.54:3300/0,v1:172.16.11.54:6789/0] [v2:172.16.11.121:3300/0,v1:172.16.11.121:6789/0] [v2:172.16.11.205:3300/0,v1:172.16.11.205:6789/0]\n\n[root@oc0-controller-1 ~]# ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-ex    inet 192.168.24.23/24 brd 192.168.24.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n6: vlan100    inet 192.168.100.96/24 brd 192.168.100.255 scope global vlan100\\       valid_lft forever preferred_lft forever\n7: vlan12    inet 172.16.12.154/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n8: vlan11    inet 172.16.11.121/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n9: vlan13    inet 172.16.13.178/24 brd 172.16.13.255 scope global vlan13\\       valid_lft forever preferred_lft forever\n10: vlan70    inet 172.17.0.23/20 brd 172.17.15.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n11: vlan1    inet 192.168.24.23/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n12: vlan14    inet 172.16.14.223/24 brd 172.16.14.255 scope global vlan14\\       valid_lft forever preferred_lft forever\n</code></pre> <p>On the oc0-ceph-0:</p> <pre><code>[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n[heat-admin@oc0-ceph-0 ~]$ sudo ip a add 172.16.11.121 dev vlan11\n[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.121/32 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre> <p>Poweroff oc0-controller-1.</p> <p>Add the new mon on oc0-ceph-0 using the old IP address:</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch daemon add mon oc0-ceph-0:172.16.11.121\nDeployed mon.oc0-ceph-0 on host 'oc0-ceph-0'\n</code></pre> <p>Check the new container in the oc0-ceph-0 node:</p> <pre><code>b581dc8bbb78  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-ceph-0...  24 seconds ago  Up 24 seconds ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-ceph-0\n</code></pre> <p>On the cephadm shell, backup the existing ceph_spec.yaml, edit the spec removing any oc0-controller-1 entry, and replacing it with oc0-ceph-0:</p> <pre><code>cp ceph_spec.yaml ceph_spec.yaml.bkp # backup the ceph_spec.yaml file\n\n[ceph: root@oc0-controller-0 specs]# diff -u ceph_spec.yaml.bkp ceph_spec.yaml\n\n--- ceph_spec.yaml.bkp  2022-07-29 15:41:34.516329643 +0000\n+++ ceph_spec.yaml      2022-07-29 15:28:26.455329643 +0000\n@@ -7,14 +7,6 @@\n - mgr\n service_type: host\n ---\n-addr: 192.168.24.12\n-hostname: oc0-controller-1\n-labels:\n-- _admin\n-- mon\n-- mgr\n-service_type: host\n----\n addr: 192.168.24.19\n hostname: oc0-controller-2\n labels:\n@@ -38,7 +30,7 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n+  - oc0-ceph-0\n   - oc0-controller-2\n service_id: mon\n service_name: mon\n@@ -47,8 +39,8 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n   - oc0-controller-2\n+  - oc0-ceph-0\n service_id: mgr\n service_name: mgr\n service_type: mgr\n</code></pre> <p>Apply the resulting spec:</p> <pre><code>ceph orch apply -i ceph_spec.yaml \n\n The result of 12 is having a new mgr deployed on the oc0-ceph-0 node, and the spec reconciled within cephadm\n\n[ceph: root@oc0-controller-0 specs]# ceph orch ls\nNAME                     PORTS  RUNNING  REFRESHED  AGE  PLACEMENT\ncrash                               4/4  5m ago     61m  *\nmgr                                 3/3  5m ago     69s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nmon                                 3/3  5m ago     70s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nosd.default_drive_group               8  2m ago     69s  oc0-ceph-0;oc0-ceph-1\n\n[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_WARN\n            1 stray host(s) with 1 daemon(s) not managed by cephadm\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 5m)\n    mgr: oc0-controller-0.xzgtvo(active, since 62m), standbys: oc0-controller-2.ahrgsk, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 42m), 8 in (since 49m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>Fix the warning by refreshing the mgr:</p> <pre><code>ceph mgr fail oc0-controller-0.xzgtvo\n</code></pre> <p>And at this point the cluster is clean:</p> <pre><code>[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 7m)\n    mgr: oc0-controller-2.ahrgsk(active, since 25s), standbys: oc0-controller-0.xzgtvo, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 44m), 8 in (since 50m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>oc0-controller-1 has been removed and powered off without leaving traces on the ceph cluster.</p> <p>The same approach and the same steps can be applied to migrate oc0-controller-2 to oc0-ceph-1.</p>"},{"location":"ceph/ceph_rbd/#screen-recording","title":"Screen Recording:","text":"<ul> <li>Externalize a TripleO deployed Ceph cluster</li> </ul>"},{"location":"ceph/ceph_rbd/#whats-next","title":"What\u2019s next","text":""},{"location":"ceph/ceph_rbd/#useful-resources","title":"Useful resources","text":"<ul> <li>cephadm - deploy additional mon(s)</li> </ul>"},{"location":"ceph/ceph_rgw/","title":"Data Plane adoption - Ceph RGW Migration","text":"<p>In this scenario, assuming Ceph is already &gt;= 5, either for HCI or dedicated Storage nodes, the RGW daemons living in the OpenStack Controller nodes will be migrated into the existing external RHEL nodes (typically the Compute nodes for an HCI environment or CephStorage nodes in the remaining use cases).</p>"},{"location":"ceph/ceph_rgw/#requirements","title":"Requirements","text":"<ul> <li>Ceph is &gt;= 5 and managed by cephadm/orchestrator</li> <li>An undercloud is still available: nodes and networks are managed by TripleO</li> </ul>"},{"location":"ceph/ceph_rgw/#ceph-daemon-cardinality","title":"Ceph Daemon Cardinality","text":"<p>Ceph 5+ applies strict constraints in the way daemons can be colocated within the same node. The resulting topology depends on the available hardware, as well as the amount of Ceph services present in the Controller nodes which are going to be retired. The following document describes the procedure required to migrate the RGW component (and keep an HA model using the Ceph Ingress daemon in a common TripleO scenario where Controller nodes represent the spec placement where the service is deployed. As a general rule, the number of services that can be migrated depends on the number of available nodes in the cluster. The following diagrams cover the distribution of the Ceph daemons on the CephStorage nodes where at least three nodes are required in a scenario that sees only RGW and RBD (no dashboard):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress <p>With dashboard, and without Manila at least four nodes are required (dashboard has no failover):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash dashboard/grafana osd rgw/ingress (free) <p>With dashboard and Manila 5 nodes minimum are required (and dashboard has no failover):</p> osd mon/mgr/crash rgw/ingress osd mon/mgr/crash rgw/ingress osd mon/mgr/crash mds/ganesha/ingress osd rgw/ingress mds/ganesha/ingress osd mds/ganesha/ingress dashboard/grafana"},{"location":"ceph/ceph_rgw/#current-status","title":"Current Status","text":"<pre><code>(undercloud) [stack@undercloud-0 ~]$ metalsmith list\n\n\n    +------------------------+    +----------------+\n    | IP Addresses           |    |  Hostname      |\n    +------------------------+    +----------------+\n    | ctlplane=192.168.24.25 |    | cephstorage-0  |\n    | ctlplane=192.168.24.10 |    | cephstorage-1  |\n    | ctlplane=192.168.24.32 |    | cephstorage-2  |\n    | ctlplane=192.168.24.28 |    | compute-0      |\n    | ctlplane=192.168.24.26 |    | compute-1      |\n    | ctlplane=192.168.24.43 |    | controller-0   |\n    | ctlplane=192.168.24.7  |    | controller-1   |\n    | ctlplane=192.168.24.41 |    | controller-2   |\n    +------------------------+    +----------------+\n</code></pre> <p>SSH into <code>controller-0</code> and check the <code>pacemaker</code> status: this will help identify the relevant information that we need to know before starting the RGW migration.</p> <pre><code>Full List of Resources:\n  * ip-192.168.24.46    (ocf:heartbeat:IPaddr2):        Started controller-0\n  * ip-10.0.0.103       (ocf:heartbeat:IPaddr2):        Started controller-1\n  * ip-172.17.1.129     (ocf:heartbeat:IPaddr2):        Started controller-2\n  * ip-172.17.3.68      (ocf:heartbeat:IPaddr2):        Started controller-0\n  * ip-172.17.4.37      (ocf:heartbeat:IPaddr2):        Started controller-1\n  * Container bundle set: haproxy-bundle\n\n[undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:\n    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-2\n    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-0\n    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-1\n</code></pre> <p>Use the <code>ip</code> command to identify the ranges of the storage networks.</p> <pre><code>[heat-admin@controller-0 ~]$ ip -o -4 a\n\n1: lo   inet 127.0.0.1/8 scope host lo\\     valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.45/24 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.46/32 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n7: br-ex    inet 10.0.0.122/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n8: vlan70   inet 172.17.5.22/24 brd 172.17.5.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n8: vlan70   inet 172.17.5.94/32 brd 172.17.5.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n9: vlan50   inet 172.17.2.140/24 brd 172.17.2.255 scope global vlan50\\      valid_lft forever preferred_lft forever\n10: vlan30  inet 172.17.3.73/24 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n10: vlan30  inet 172.17.3.68/32 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n11: vlan20  inet 172.17.1.88/24 brd 172.17.1.255 scope global vlan20\\       valid_lft forever preferred_lft forever\n12: vlan40  inet 172.17.4.24/24 brd 172.17.4.255 scope global vlan40\\       valid_lft forever preferred_lft forever\n</code></pre> <p>In this example:</p> <ul> <li>vlan30 represents the Storage Network, where the new RGW instances should be   started on the CephStorage nodes</li> <li>br-ex represents the External Network, which is where in the current   environment, haproxy has the frontend VIP assigned</li> </ul>"},{"location":"ceph/ceph_rgw/#prerequisite-check-the-frontend-network-controller-nodes","title":"Prerequisite: check the frontend network (Controller nodes)","text":"<p>Identify the network that we previously had in haproxy and propagate it (via TripleO) to the CephStorage nodes. This network is used to reserve a new VIP that will be owned by Ceph and used as the entry point for the RGW service.</p> <p>ssh into <code>controller-0</code> and check the current HaProxy configuration until we find <code>ceph_rgw</code> section:</p> <pre><code>$ less /var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg\n\n...\n...\nlisten ceph_rgw\n  bind 10.0.0.103:8080 transparent\n  bind 172.17.3.68:8080 transparent\n  mode http\n  balance leastconn\n  http-request set-header X-Forwarded-Proto https if { ssl_fc }\n  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }\n  http-request set-header X-Forwarded-Port %[dst_port]\n  option httpchk GET /swift/healthcheck\n  option httplog\n  option forwardfor\n  server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2\n  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2\n  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2\n</code></pre> <p>Double check the network used as HaProxy frontend:</p> <pre><code>[controller-0]$ ip -o -4 a\n\n...\n7: br-ex    inet 10.0.0.106/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n...\n</code></pre> <p>As described in the previous section, the check on controller-0 shows that we are exposing the services using the external network, which is not present in the CephStorage nodes, and we need to propagate it via TripleO.</p>"},{"location":"ceph/ceph_rgw/#propagate-the-haproxy-frontend-network-to-cephstorage-nodes","title":"Propagate the <code>HaProxy</code> frontend network to <code>CephStorage</code> nodes","text":"<p>Change the nic template used to define the ceph-storage network interfaces and add the new config section.</p> <pre><code>---\nnetwork_config:\n- type: interface\n  name: nic1\n  use_dhcp: false\n  dns_servers: {{ ctlplane_dns_nameservers }}\n  addresses:\n  - ip_netmask: {{ ctlplane_ip }}/{{ ctlplane_subnet_cidr }}\n  routes: {{ ctlplane_host_routes }}\n- type: vlan\n  vlan_id: {{ storage_mgmt_vlan_id }}\n  device: nic1\n  addresses:\n  - ip_netmask: {{ storage_mgmt_ip }}/{{ storage_mgmt_cidr }}\n  routes: {{ storage_mgmt_host_routes }}\n- type: interface\n  name: nic2\n  use_dhcp: false\n  defroute: false\n- type: vlan\n  vlan_id: {{ storage_vlan_id }}\n  device: nic2\n  addresses:\n  - ip_netmask: {{ storage_ip }}/{{ storage_cidr }}\n  routes: {{ storage_host_routes }}\n- type: ovs_bridge\n  name: {{ neutron_physical_bridge_name }}\n  dns_servers: {{ ctlplane_dns_nameservers }}\n  domain: {{ dns_search_domains }}\n  use_dhcp: false\n  addresses:\n  - ip_netmask: {{ external_ip }}/{{ external_cidr }}\n  routes: {{ external_host_routes }}\n  members:\n  - type: interface\n    name: nic3\n    primary: true\n</code></pre> <p>In addition, add the External Network to the <code>baremetal.yaml</code> file used by metalsmith and run the <code>overcloud node provision</code> command passing the <code>--network-config</code> option:</p> <pre><code>- name: CephStorage\n  count: 3\n  hostname_format: cephstorage-%index%\n  instances:\n  - hostname: cephstorage-0\n  name: ceph-0\n  - hostname: cephstorage-1\n  name: ceph-1\n  - hostname: cephstorage-2\n  name: ceph-2\n  defaults:\n  profile: ceph-storage\n  network_config:\n      template: /home/stack/composable_roles/network/nic-configs/ceph-storage.j2\n  networks:\n  - network: ctlplane\n      vif: true\n  - network: storage\n  - network: storage_mgmt\n  - network: external\n</code></pre> <pre><code>(undercloud) [stack@undercloud-0]$\n\nopenstack overcloud node provision\n   -o overcloud-baremetal-deployed-0.yaml\n   --stack overcloud\n   --network-config -y\n  $PWD/network/baremetal_deployment.yaml\n</code></pre> <p>Check the new network on the <code>CephStorage</code> nodes:</p> <pre><code>[root@cephstorage-0 ~]# ip -o -4 a\n\n1: lo   inet 127.0.0.1/8 scope host lo\\     valid_lft forever preferred_lft forever\n2: enp1s0   inet 192.168.24.54/24 brd 192.168.24.255 scope global enp1s0\\       valid_lft forever preferred_lft forever\n11: vlan40  inet 172.17.4.43/24 brd 172.17.4.255 scope global vlan40\\       valid_lft forever preferred_lft forever\n12: vlan30  inet 172.17.3.23/24 brd 172.17.3.255 scope global vlan30\\       valid_lft forever preferred_lft forever\n14: br-ex   inet 10.0.0.133/24 brd 10.0.0.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n</code></pre> <p>And now it\u2019s time to start migrating the RGW backends and build the ingress on top of them.</p>"},{"location":"ceph/ceph_rgw/#migrate-the-rgw-backends","title":"Migrate the RGW backends","text":"<p>To match the cardinality diagram we use cephadm labels to refer to a group of nodes where a given daemon type should be deployed.</p> <p>Add the RGW label to the cephstorage nodes:</p> <pre><code>for i in 0 1 2; {\n    ceph orch host label add cephstorage-$i rgw;\n}\n</code></pre> <pre><code>[ceph: root@controller-0 /]#\n\nfor i in 0 1 2; {\n    ceph orch host label add cephstorage-$i rgw;\n}\n\nAdded label rgw to host cephstorage-0\nAdded label rgw to host cephstorage-1\nAdded label rgw to host cephstorage-2\n\n[ceph: root@controller-0 /]# ceph orch host ls\n\nHOST        ADDR        LABELS          STATUS\ncephstorage-0  192.168.24.54  osd rgw\ncephstorage-1  192.168.24.44  osd rgw\ncephstorage-2  192.168.24.30  osd rgw\ncontroller-0   192.168.24.45  _admin mon mgr\ncontroller-1   192.168.24.11  _admin mon mgr\ncontroller-2   192.168.24.38  _admin mon mgr\n\n6 hosts in cluster\n</code></pre> <p>During the overcloud deployment, RGW is applied at step2 (external_deployment_steps), and a cephadm compatible spec is generated in <code>/home/ceph-admin/specs/rgw</code> from the ceph_mkspec ansible module. Find and patch the RGW spec, specifying the right placement using the labels approach, and change the rgw backend port to 8090 to avoid conflicts with the Ceph Ingress Daemon (*)</p> <pre><code>[root@controller-0 heat-admin]# cat rgw\n\nnetworks:\n- 172.17.3.0/24\nplacement:\n  hosts:\n  - controller-0\n  - controller-1\n  - controller-2\nservice_id: rgw\nservice_name: rgw.rgw\nservice_type: rgw\nspec:\n  rgw_frontend_port: 8080\n  rgw_realm: default\n  rgw_zone: default\n</code></pre> <p>Patch the spec replacing controller nodes with the label key</p> <pre><code>---\nnetworks:\n- 172.17.3.0/24\nplacement:\n  label: rgw\nservice_id: rgw\nservice_name: rgw.rgw\nservice_type: rgw\nspec:\n  rgw_frontend_port: 8090\n  rgw_realm: default\n  rgw_zone: default\n</code></pre> <p>(*) cephadm_check_port</p> <p>Apply the new RGW spec using the orchestrator CLI:</p> <pre><code>$ cephadm shell -m /home/ceph-admin/specs/rgw\n$ cephadm shell -- ceph orch apply -i /mnt/rgw\n</code></pre> <p>Which triggers the redeploy:</p> <pre><code>...\nosd.9                       cephstorage-2\nrgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090   starting\nrgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090   starting\nrgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090   starting\nrgw.rgw.controller-1.eyvrzw   controller-1   172.17.3.146:8080  running (5h)\nrgw.rgw.controller-2.navbxa   controller-2   172.17.3.66:8080   running (5h)\n\n...\nosd.9                       cephstorage-2\nrgw.rgw.cephstorage-0.wsjlgx  cephstorage-0  172.17.3.23:8090  running (19s)\nrgw.rgw.cephstorage-1.qynkan  cephstorage-1  172.17.3.26:8090  running (16s)\nrgw.rgw.cephstorage-2.krycit  cephstorage-2  172.17.3.81:8090  running (13s)\n</code></pre> <p>At this point, we need to make sure that the new RGW backends are reachable on the new ports, but we\u2019re going to enable an IngressDaemon on port 8080 later in the process. For this reason, ssh on each RGW node (the CephStorage nodes) and add the iptables rule to allow connections to both 8080 and 8090 ports in the CephStorage nodes.</p> <pre><code>iptables -I INPUT -p tcp -m tcp --dport 8080 -m conntrack --ctstate NEW -m comment --comment \"ceph rgw ingress\" -j ACCEPT\n\niptables -I INPUT -p tcp -m tcp --dport 8090 -m conntrack --ctstate NEW -m comment --comment \"ceph rgw backends\" -j ACCEPT\n\nfor port in 8080 8090; { \n    for i in 25 10 32; {\n       ssh heat-admin@192.168.24.$i sudo iptables -I INPUT \\\n       -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \\\n       -j ACCEPT;\n   }\n}\n</code></pre> <p>From a Controller node (e.g. controller-0) try to reach (curl) the rgw backends:</p> <pre><code>for i in 26 23 81; do {\n    echo \"----\"\n    curl 172.17.3.$i:8090;\n    echo \"----\"\n    echo\ndone\n</code></pre> <p>And you should observe the following:</p> <pre><code>----\nQuery 172.17.3.23\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n\n----\nQuery 172.17.3.26\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n\n----\nQuery 172.17.3.81\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;\n---\n</code></pre>"},{"location":"ceph/ceph_rgw/#note","title":"NOTE","text":"<p>In case RGW backends are migrated in the CephStorage nodes, there\u2019s no \u201cinternalAPI\u201d network(this is not true in the case of HCI). Reconfig the RGW keystone endpoint, pointing to the external Network that has been propagated (see the previous section)</p> <pre><code>[ceph: root@controller-0 /]# ceph config dump | grep keystone\nglobal   basic rgw_keystone_url  http://172.16.1.111:5000\n\n[ceph: root@controller-0 /]# ceph config set global rgw_keystone_url http://10.0.0.103:5000\n</code></pre>"},{"location":"ceph/ceph_rgw/#deploy-a-ceph-ingressdaemon","title":"Deploy a Ceph IngressDaemon","text":"<p><code>HaProxy</code> is managed by TripleO via <code>Pacemaker</code>: the three running instances at this point will point to the old RGW backends, resulting in a wrong, not working configuration. Since we\u2019re going to deploy the Ceph Ingress Daemon, the first thing to do is remove the existing <code>ceph_rgw</code> config, clean up the config created by TripleO and restart the service to make sure other services are not affected by this change.</p> <p>ssh  on each Controller node and remove the following is the section from <code>/var/lib/config-data/puppet-generated/haproxy/etc/haproxy/haproxy.cfg</code>:</p> <pre><code>listen ceph_rgw\n  bind 10.0.0.103:8080 transparent\n  mode http\n  balance leastconn\n  http-request set-header X-Forwarded-Proto https if { ssl_fc }\n  http-request set-header X-Forwarded-Proto http if !{ ssl_fc }\n  http-request set-header X-Forwarded-Port %[dst_port]\n  option httpchk GET /swift/healthcheck\n  option httplog\n  option forwardfor\n   server controller-0.storage.redhat.local 172.17.3.73:8080 check fall 5 inter 2000 rise 2\n  server controller-1.storage.redhat.local 172.17.3.146:8080 check fall 5 inter 2000 rise 2\n  server controller-2.storage.redhat.local 172.17.3.156:8080 check fall 5 inter 2000 rise 2\n</code></pre> <p>Restart <code>haproxy-bundle</code> and make sure it\u2019s started:</p> <pre><code>[root@controller-0 ~]# sudo pcs resource restart haproxy-bundle\nhaproxy-bundle successfully restarted\n\n\n[root@controller-0 ~]# sudo pcs status | grep haproxy\n\n  * Container bundle set: haproxy-bundle [undercloud-0.ctlplane.redhat.local:8787/rh-osbs/rhosp17-openstack-haproxy:pcmklatest]:\n    * haproxy-bundle-podman-0   (ocf:heartbeat:podman):  Started controller-0\n    * haproxy-bundle-podman-1   (ocf:heartbeat:podman):  Started controller-1\n    * haproxy-bundle-podman-2   (ocf:heartbeat:podman):  Started controller-2\n</code></pre> <p>Double check no process is bound to 8080 anymore\u201d</p> <pre><code>[root@controller-0 ~]# ss -antop | grep 8080\n[root@controller-0 ~]#\n</code></pre> <p>And the swift CLI should fail at this point:</p> <pre><code>(overcloud) [root@cephstorage-0 ~]# swift list\n\nHTTPConnectionPool(host='10.0.0.103', port=8080): Max retries exceeded with url: /swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7fc41beb0430&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n</code></pre> <p>Now we can start deploying the Ceph IngressDaemon on the CephStorage nodes.</p> <p>Set the required images for both HaProxy and Keepalived</p> <pre><code>[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_haproxy quay.io/ceph/haproxy:2.3\n\n[ceph: root@controller-0 /]# ceph config set mgr mgr/cephadm/container_image_keepalived quay.io/ceph/keepalived:2.1.5\n</code></pre> <p>Prepare the ingress spec and mount it to cephadm:</p> <pre><code>$ sudo vim /home/ceph-admin/specs/rgw_ingress\n</code></pre> <p>and paste the following content:</p> <pre><code>---\nservice_type: ingress\nservice_id: rgw.rgw\nplacement:\n  label: rgw\nspec:\n  backend_service: rgw.rgw\n  virtual_ip: 10.0.0.89/24\n  frontend_port: 8080\n  monitor_port: 8898\n  virtual_interface_networks:\n    - 10.0.0.0/24\n</code></pre> <p>Mount the generated spec and apply it using the orchestrator CLI:</p> <pre><code>$ cephadm shell -m /home/ceph-admin/specs/rgw_ingress\n$ cephadm shell -- ceph orch apply -i /mnt/rgw_ingress\n</code></pre> <p>Wait until the ingress is deployed and query the resulting endpoint:</p> <pre><code>[ceph: root@controller-0 /]# ceph orch ls\n\nNAME                    PORTS               RUNNING  REFRESHED  AGE  PLACEMENT\ncrash                                           6/6  6m ago     3d   *\ningress.rgw.rgw         10.0.0.89:8080,8898     6/6  37s ago    60s  label:rgw\nmds.mds                   3/3  6m ago   3d   controller-0;controller-1;controller-2\nmgr                       3/3  6m ago   3d   controller-0;controller-1;controller-2\nmon                       3/3  6m ago   3d   controller-0;controller-1;controller-2\nosd.default_drive_group   15  37s ago   3d   cephstorage-0;cephstorage-1;cephstorage-2\nrgw.rgw   ?:8090          3/3  37s ago  4m   label:rgw\n</code></pre> <pre><code>[ceph: root@controller-0 /]# curl  10.0.0.89:8080\n\n---\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName&gt;&lt;/DisplayName&gt;&lt;/Owner&gt;&lt;Buckets&gt;&lt;/Buckets&gt;&lt;/ListAllMyBucketsResult&gt;[ceph: root@controller-0 /]#\n\u2014\n</code></pre> <p>The result above shows that we\u2019re able to reach the backend from the IngressDaemon, which means we\u2019re almost ready to interact with it using the swift CLI.</p>"},{"location":"ceph/ceph_rgw/#update-the-object-store-endpoints","title":"Update the object-store endpoints","text":"<p>The endpoints still point to the old VIP owned by pacemaker, but given it\u2019s still used by other services and we reserved a new VIP on the same network, before any other action we should update the object-store endpoint.</p> <p>List the current endpoints:</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object\n\n| 1326241fb6b6494282a86768311f48d1 | regionOne | swift      | object-store   | True | internal  | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |\n| 8a34817a9d3443e2af55e108d63bb02b | regionOne | swift      | object-store   | True | public    | http://10.0.0.103:8080/swift/v1/AUTH_%(project_id)s  |\n| fa72f8b8b24e448a8d4d1caaeaa7ac58 | regionOne | swift      | object-store   | True | admin     | http://172.17.3.68:8080/swift/v1/AUTH_%(project_id)s |\n</code></pre> <p>Update the endpoints pointing to the Ingress VIP:</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ openstack endpoint set --url \"http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s\" 95596a2d92c74c15b83325a11a4f07a3\n\n(overcloud) [stack@undercloud-0 ~]$ openstack endpoint list | grep object-store\n| 6c7244cc8928448d88ebfad864fdd5ca | regionOne | swift      | object-store   | True | internal  | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |\n| 95596a2d92c74c15b83325a11a4f07a3 | regionOne | swift      | object-store   | True | public    | http://10.0.0.89:8080/swift/v1/AUTH_%(project_id)s   |\n| e6d0599c5bf24a0fb1ddf6ecac00de2d | regionOne | swift      | object-store   | True | admin     | http://172.17.3.79:8080/swift/v1/AUTH_%(project_id)s |\n</code></pre> <p>And repeat the same action for both internal and admin. Test the migrated service.</p> <pre><code>(overcloud) [stack@undercloud-0 ~]$ swift list --debug\n\nDEBUG:swiftclient:Versionless auth_url - using http://10.0.0.115:5000/v3 as endpoint\nDEBUG:keystoneclient.auth.identity.v3.base:Making authentication request to http://10.0.0.115:5000/v3/auth/tokens\nDEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 10.0.0.115:5000\nDEBUG:urllib3.connectionpool:http://10.0.0.115:5000 \"POST /v3/auth/tokens HTTP/1.1\" 201 7795\nDEBUG:keystoneclient.auth.identity.v3.base:{\"token\": {\"methods\": [\"password\"], \"user\": {\"domain\": {\"id\": \"default\", \"name\": \"Default\"}, \"id\": \"6f87c7ffdddf463bbc633980cfd02bb3\", \"name\": \"admin\", \"password_expires_at\": null}, \n\n\n...\n...\n...\n\nDEBUG:swiftclient:REQ: curl -i http://10.0.0.89:8080/swift/v1/AUTH_852f24425bb54fa896476af48cbe35d3?format=json -X GET -H \"X-Auth-Token: gAAAAABj7KHdjZ95syP4c8v5a2zfXckPwxFQZYg0pgWR42JnUs83CcKhYGY6PFNF5Cg5g2WuiYwMIXHm8xftyWf08zwTycJLLMeEwoxLkcByXPZr7kT92ApT-36wTfpi-zbYXd1tI5R00xtAzDjO3RH1kmeLXDgIQEVp0jMRAxoVH4zb-DVHUos\" -H \"Accept-Encoding: gzip\"\nDEBUG:swiftclient:RESP STATUS: 200 OK\nDEBUG:swiftclient:RESP HEADERS: {'content-length': '2', 'x-timestamp': '1676452317.72866', 'x-account-container-count': '0', 'x-account-object-count': '0', 'x-account-bytes-used': '0', 'x-account-bytes-used-actual': '0', 'x-account-storage-policy-default-placement-container-count': '0', 'x-account-storage-policy-default-placement-object-count': '0', 'x-account-storage-policy-default-placement-bytes-used': '0', 'x-account-storage-policy-default-placement-bytes-used-actual': '0', 'x-trans-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'x-openstack-request-id': 'tx00000765c4b04f1130018-0063eca1dd-1dcba-default', 'accept-ranges': 'bytes', 'content-type': 'application/json; charset=utf-8', 'date': 'Wed, 15 Feb 2023 09:11:57 GMT'}\nDEBUG:swiftclient:RESP BODY: b'[]'\n</code></pre> <p>Run tempest tests against object-storage:</p> <pre><code>(overcloud) [stack@undercloud-0 tempest-dir]$  tempest run --regex tempest.api.object_storage\n...\n...\n...\n======\nTotals\n======\nRan: 141 tests in 606.5579 sec.\n - Passed: 128\n - Skipped: 13\n - Expected Fail: 0\n - Unexpected Success: 0\n - Failed: 0\nSum of execute time for each test: 657.5183 sec.\n\n==============\nWorker Balance\n==============\n - Worker 0 (1 tests) =&gt; 0:10:03.400561\n - Worker 1 (2 tests) =&gt; 0:00:24.531916\n - Worker 2 (4 tests) =&gt; 0:00:10.249889\n - Worker 3 (30 tests) =&gt; 0:00:32.730095\n - Worker 4 (51 tests) =&gt; 0:00:26.246044\n - Worker 5 (6 tests) =&gt; 0:00:20.114803\n - Worker 6 (20 tests) =&gt; 0:00:16.290323\n - Worker 7 (27 tests) =&gt; 0:00:17.103827\n</code></pre>"},{"location":"ceph/ceph_rgw/#additional-resources","title":"Additional Resources","text":"<p>A screen recording is available here.</p>"},{"location":"contributing/development_environment/","title":"Development environment","text":"<p>This is a guide for an install_yamls based Adoption environment with network isolation as an alternative to the CRC and Vagrant TripleO Standalone development environment guide.</p> <p>The Adoption development environment utilizes install_yamls for CRC VM creation and for creation of the VM that hosts the original Wallaby OpenStack in Standalone configuration.</p>"},{"location":"contributing/development_environment/#environment-prep","title":"Environment prep","text":"<p>Get install_yamls:</p> <pre><code>git clone https://github.com/openstack-k8s-operators/install_yamls.git\n</code></pre> <p>Install tools for operator development:</p> <pre><code>cd install_yamls/devsetup\nmake download_tools\n</code></pre> <p>If you want a less intrusive alternative (Go from RPM rather than upstream etc.) that allows for basic testing, make sure to at least do the following:</p> <pre><code>sudo dnf -y install \\\n    git \\\n    golang \\\n    guestfs-tools \\\n    dbus-x11 \\\n    libvirt \\\n    make\n\ngo env -w GOPROXY=\"https://proxy.golang.org,direct\"\nGO111MODULE=on go install sigs.k8s.io/kustomize/kustomize/v5@latest\n</code></pre>"},{"location":"contributing/development_environment/#deployment-of-crc-with-network-isolation","title":"Deployment of CRC with network isolation","text":"<p><pre><code>cd install_yamls/devsetup\nPULL_SECRET=$HOME/pull-secret.txt CPUS=12 MEMORY=40000 DISK=100 make crc\n\neval $(crc oc-env)\noc login -u kubeadmin -p 12345678 https://api.crc.testing:6443\n\nmake crc_attach_default_interface\n\ncd ..  # back to install_yamls\nmake crc_storage\nmake input\nmake openstack\n</code></pre> Use the install_yamls devsetup to create a virtual machine connected to the isolated networks.</p> <p>Create the edpm-compute-0 virtual machine. <pre><code>cd install_yamls/devsetup\nEDPM_COMPUTE_VCPUS=8 EDPM_COMPUTE_RAM=20 EDPM_COMPUTE_DISK_SIZE=70 make edpm_compute\n\nssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100\n</code></pre></p>"},{"location":"contributing/development_environment/#deployment-of-wallaby-standalone-with-network-isolation","title":"Deployment of Wallaby Standalone with network isolation","text":"<p>The steps in this section should be run on the edpm-compute-0 virtual machine so that it's configured to host TripleO Standalone with Ceph.</p> <p>Configure the repositories and install the necessary packages. <pre><code>sudo dnf remove -y epel-release\nsudo dnf update -y\nsudo dnf install -y vim git curl util-linux lvm2 tmux wget\nURL=https://trunk.rdoproject.org/centos9/component/tripleo/current/\nRPM_NAME=$(curl $URL | grep python3-tripleo-repos | sed -e 's/&lt;[^&gt;]*&gt;//g' | awk 'BEGIN { FS = \".rpm\" } ; { print $1 }')\nRPM=$RPM_NAME.rpm\nsudo dnf install -y $URL$RPM\nsudo -E tripleo-repos -b wallaby current-tripleo-dev ceph --stream\nsudo dnf repolist\nsudo dnf update -y\nsudo dnf install -y podman python3-tripleoclient util-linux lvm2 cephadm\n</code></pre> Set the hostname. <pre><code>sudo hostnamectl set-hostname standalone.localdomain\nsudo hostnamectl set-hostname standalone.localdomain --transient\n</code></pre> Create a containers-prepare-parameters.yaml file. <pre><code>openstack tripleo container image prepare default \\\n  --output-env-file $HOME/containers-prepare-parameters.yaml\n</code></pre></p>"},{"location":"contributing/development_environment/#networking","title":"Networking","text":"<p>Use os-net-config to add VLAN interfaces which connect edpm-compute-0 to the isolated networks configured by <code>install_yamls</code>. <pre><code>export GATEWAY=192.168.122.1\nexport CTLPLANE_IP=192.168.122.100\nexport INTERNAL_IP=$(sed -e 's/192.168.122/172.17.0/' &lt;&lt;&lt;\"$CTLPLANE_IP\")\nexport STORAGE_IP=$(sed -e 's/192.168.122/172.18.0/' &lt;&lt;&lt;\"$CTLPLANE_IP\")\nexport STORAGE_MGMT_IP=$(sed -e 's/192.168.122/172.20.0/' &lt;&lt;&lt;\"$CTLPLANE_IP\")\nexport TENANT_IP=$(sed -e 's/192.168.122/172.19.0/' &lt;&lt;&lt;\"$CTLPLANE_IP\")\nexport EXTERNAL_IP=$(sed -e 's/192.168.122/172.21.0/' &lt;&lt;&lt;\"$CTLPLANE_IP\")\n\nsudo mkdir -p /etc/os-net-config\ncat &lt;&lt; EOF | sudo tee /etc/os-net-config/config.yaml\nnetwork_config:\n- type: ovs_bridge\n  name: br-ctlplane\n  mtu: 1500\n  use_dhcp: false\n  dns_servers:\n  - $GATEWAY\n  domain: []\n  addresses:\n  - ip_netmask: $CTLPLANE_IP/24\n  routes:\n  - ip_netmask: 0.0.0.0/0\n    next_hop: $GATEWAY\n  members:\n  - type: interface\n    name: nic1\n    mtu: 1500\n    # force the MAC address of the bridge to this interface\n    primary: true\n\n  # external\n  - type: vlan\n    mtu: 1500\n    vlan_id: 44\n    addresses:\n    - ip_netmask: $EXTERNAL_IP/24\n    routes: []\n\n  # internal\n  - type: vlan\n    mtu: 1500\n    vlan_id: 20\n    addresses:\n    - ip_netmask: $INTERNAL_IP/24\n    routes: []\n\n  # storage\n  - type: vlan\n    mtu: 1500\n    vlan_id: 21\n    addresses:\n    - ip_netmask: $STORAGE_IP/24\n    routes: []\n\n  # storage_mgmt\n  - type: vlan\n    mtu: 1500\n    vlan_id: 23\n    addresses:\n    - ip_netmask: $STORAGE_MGMT_IP/24\n    routes: []\n\n  # tenant\n  - type: vlan\n    mtu: 1500\n    vlan_id: 22\n    addresses:\n    - ip_netmask: $TENANT_IP/24\n    routes: []\nEOF\n\ncat &lt;&lt; EOF | sudo tee /etc/cloud/cloud.cfg.d/99-edpm-disable-network-config.cfg\nnetwork:\n  config: disabled\nEOF\n\nsudo systemctl enable network\nsudo os-net-config -c /etc/os-net-config/config.yaml\n</code></pre></p> <p>The isolated networks from os-net-config config file above will be lost when <code>openstack tripleo deploy</code> is run because the default os-net-config template only has the Neutron public interface as a member. To prevent this, copy this standalone.j2 template file (which retains the VLANs above) into tripleo-ansible's <code>tripleo_network_config</code> role. <pre><code>sudo cp standalone.j2 /usr/share/ansible/roles/tripleo_network_config/templates/standalone.j2\n</code></pre></p> <p>Assign VIPs to the networks created when os-net-config was run. The tenant network on vlan22 does not require a VIP. <pre><code>sudo ip addr add 172.17.0.2/32 dev vlan20\nsudo ip addr add 172.18.0.2/32 dev vlan21\nsudo ip addr add 172.20.0.2/32 dev vlan23\nsudo ip addr add 172.21.0.2/32 dev vlan44\n</code></pre></p>"},{"location":"contributing/development_environment/#ceph","title":"Ceph","text":"<p>These steps are based on TripleO Standalone to configure Ceph on the Standalone node to simulate an HCI or internal Ceph adoption. Ceph will be configured to use the Storage network (vlan21) and Storage Management network (vlan23). The storage management network, is not configured by default in an NG environment and does not need to be accessed by the NG environment as it is only used by Ceph (AKA the <code>cluster_network</code>) to make OSD replicas and NG will not be deploying Ceph. Post adoption this network will remain isolated and the Ceph cluster may be considered external.</p> <p>Assign the IP from vlan21 to a variable representing the Ceph IP. <pre><code>export CEPH_IP=172.18.0.100\n</code></pre> Create a block device with logical volumes to be used as an OSD. <pre><code>sudo dd if=/dev/zero of=/var/lib/ceph-osd.img bs=1 count=0 seek=7G\nsudo losetup /dev/loop3 /var/lib/ceph-osd.img\nsudo pvcreate /dev/loop3\nsudo vgcreate vg2 /dev/loop3\nsudo lvcreate -n data-lv2 -l +100%FREE vg2\n</code></pre> Create an OSD spec file which references the block device. <pre><code>cat &lt;&lt;EOF &gt; $HOME/osd_spec.yaml\ndata_devices:\n  paths:\n    - /dev/vg2/data-lv2\nEOF\n</code></pre> Use the Ceph IP and OSD spec file to create a Ceph spec file which will describe the Ceph cluster in a format cephadm can parse. <pre><code>sudo openstack overcloud ceph spec \\\n   --standalone \\\n   --mon-ip $CEPH_IP \\\n   --osd-spec $HOME/osd_spec.yaml \\\n   --output $HOME/ceph_spec.yaml\n</code></pre> Create the ceph-admin user by passing the Ceph spec created earlier. <pre><code>sudo openstack overcloud ceph user enable \\\n   --standalone \\\n   $HOME/ceph_spec.yaml\n</code></pre> Though Ceph will be configured to run on a single host via the --single-host-defaults option, this deployment only has a single OSD so it cannot replicate data even on the same host. Create an initial Ceph configuration to disable replication: <pre><code>cat &lt;&lt;EOF &gt; $HOME/initial_ceph.conf\n[global]\nosd pool default size = 1\n[mon]\nmon_warn_on_pool_no_redundancy = false\nEOF\n</code></pre> Use the files created in the previous steps to install Ceph. Use this network_data.yaml file so that Ceph uses the isolated networks for storage and storage management.</p> <p><pre><code>sudo openstack overcloud ceph deploy \\\n     --mon-ip $CEPH_IP \\\n     --ceph-spec $HOME/ceph_spec.yaml \\\n     --config $HOME/initial_ceph.conf \\\n     --standalone \\\n     --single-host-defaults \\\n     --skip-hosts-config \\\n     --skip-container-registry-config \\\n     --skip-user-create \\\n     --network-data network_data.yaml \\\n     --ntp-server clock.corp.redhat.com \\\n     --output $HOME/deployed_ceph.yaml\n</code></pre> Ceph should now be installed. Use <code>sudo cephadm shell -- ceph -s</code> to confirm the Ceph cluster health.</p>"},{"location":"contributing/development_environment/#openstack","title":"OpenStack","text":"<p>Use the files created in the previous steps including this network_data.yaml file and this deployed_network.yaml file. The deployed_network.yaml file hard codes the IPs and VIPs configured from the Networking section.</p> <p>Create standalone_parameters.yaml file and deploy standalone OpenStack using the following commands.</p> <pre><code>export NEUTRON_INTERFACE=eth0\nexport CTLPLANE_IP=192.168.122.100\nexport CTLPLANE_VIP=192.168.122.99\nexport CIDR=24\nexport DNS_SERVERS=192.168.122.1\nexport NTP_SERVER=clock.corp.redhat.com\nexport GATEWAY=192.168.122.1\nexport BRIDGE=\"br-ctlplane\"\n\ncat &lt;&lt;EOF &gt; standalone_parameters.yaml\nparameter_defaults:\n  CloudName: $CTLPLANE_IP\n  ControlPlaneStaticRoutes:\n    - ip_netmask: 0.0.0.0/0\n      next_hop: $GATEWAY\n      default: true\n  Debug: true\n  DeploymentUser: $USER\n  DnsServers: $DNS_SERVERS\n  NtpServer: $NTP_SERVER\n  # needed for vip &amp; pacemaker\n  KernelIpNonLocalBind: 1\n  DockerInsecureRegistryAddress:\n  - $CTLPLANE_IP:8787\n  NeutronPublicInterface: $NEUTRON_INTERFACE\n  # domain name used by the host\n  NeutronDnsDomain: localdomain\n  # re-use ctlplane bridge for public net\n  NeutronBridgeMappings: datacentre:$BRIDGE\n  NeutronPhysicalBridge: $BRIDGE\n  StandaloneEnableRoutedNetworks: false\n  StandaloneHomeDir: $HOME\n  InterfaceLocalMtu: 1500\n  # Needed if running in a VM\n  NovaComputeLibvirtType: qemu\n  ValidateGatewaysIcmp: false\n  ValidateControllersIcmp: false\nEOF\n\nsudo openstack tripleo deploy \\\n  --templates /usr/share/openstack-tripleo-heat-templates \\\n  --standalone-role Standalone \\\n  -e /usr/share/openstack-tripleo-heat-templates/environments/standalone/standalone-tripleo.yaml \\\n  -e /usr/share/openstack-tripleo-heat-templates/environments/low-memory-usage.yaml \\\n  -e ~/containers-prepare-parameters.yaml \\\n  -e standalone_parameters.yaml \\\n  -e /usr/share/openstack-tripleo-heat-templates/environments/cephadm/cephadm.yaml \\\n  -e ~/deployed_ceph.yaml \\\n  -e /usr/share/openstack-tripleo-heat-templates/environments/deployed-network-environment.yaml \\\n  -e deployed_network.yaml \\\n  -r /usr/share/openstack-tripleo-heat-templates/roles/Standalone.yaml \\\n  -n network_data.yaml \\\n  --local-ip=$CTLPLANE_IP/$CIDR \\\n  --control-virtual-ip=$CTLPLANE_VIP \\\n  --output-dir $HOME\n</code></pre>"},{"location":"contributing/development_environment/#snapshotrevert","title":"Snapshot/revert","text":"<p>When the deployment of the Standalone OpenStack is finished, it's a good time to snapshot the machine, so that multiple Adoption attempts can be done without having to deploy from scratch.</p> <pre><code># Virtiofs share prevents snapshotting, detach it first.\nsudo virsh detach-device-alias edpm-compute-0 fs0 --live\n\nsudo virsh snapshot-create-as --atomic --domain edpm-compute-0 --name clean\n</code></pre> <p>And when you wish to revert the Standalone deployment to the snapshotted state:</p> <pre><code>sudo virsh snapshot-revert --domain edpm-compute-0 --name clean\n</code></pre> <p>Similar snapshot could be done for the CRC virtual machine, but the developer environment reset on CRC side can be done sufficiently via the install_yamls <code>*_cleanup</code> targets.</p>"},{"location":"contributing/development_environment/#create-a-workload-to-adopt","title":"Create a workload to adopt","text":"<p>For this example we'll upload a Glance image and confirm it's using the Ceph cluster. Later you can adopt the Glance image in the NG deployment.</p> <p>Download a cirros image and convert it to raw format for Ceph. <pre><code>IMG=cirros-0.5.2-x86_64-disk.img\nURL=http://download.cirros-cloud.net/0.5.2/$IMG\nRAW=$(echo $IMG | sed s/img/raw/g)\ncurl -L -# $URL &gt; $IMG\nqemu-img convert -f qcow2 -O raw $IMG $RAW\n</code></pre> Upload the image to Glance. <pre><code>export OS_CLOUD=standalone\nopenstack image create cirros --disk-format=raw --container-format=bare &lt; $RAW\n</code></pre> Confirm the image UUID can be seen in Ceph's images pool. <pre><code>sudo cephadm shell -- rbd -p images ls -l\n</code></pre></p>"},{"location":"contributing/development_environment/#performing-the-data-plane-adoption","title":"Performing the Data Plane Adoption","text":"<p>The development environment is now set up, you can go to the Adoption documentation and perform adoption manually, or run the test suite against your environment.</p>"},{"location":"contributing/development_environment/#experimenting-with-an-additional-compute-node","title":"Experimenting with an additional compute node","text":"<p>The following is not on the critical path of preparing the development environment for Adoption, but it shows how to make the environment work with an additional compute node VM.</p> <p>The remaining steps should be completed on the hypervisor hosting crc and edpm-compute-0.</p>"},{"location":"contributing/development_environment/#deploy-ng-control-plane-with-ceph","title":"Deploy NG Control Plane with Ceph","text":"<p>Export the Ceph configuration from edpm-compute-0 into a secret. <pre><code>SSH=$(ssh -i ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa root@192.168.122.100)\nKEY=$($SSH \"cat /etc/ceph/ceph.client.openstack.keyring | base64 -w 0\")\nCONF=$($SSH \"cat /etc/ceph/ceph.conf | base64 -w 0\")\n\ncat &lt;&lt;EOF &gt; ceph_secret.yaml\napiVersion: v1\ndata:\n  ceph.client.openstack.keyring: $KEY\n  ceph.conf: $CONF\nkind: Secret\nmetadata:\n  name: ceph-conf-files\n  namespace: openstack\ntype: Opaque\nEOF\n\noc create -f ceph_secret.yaml\n</code></pre> Deploy the NG control plane with Ceph as backend for Glance and Cinder. As described in the install_yamls README, use the sample config located at https://github.com/openstack-k8s-operators/openstack-operator/blob/main/config/samples/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml but make sure to replace the <code>_FSID_</code> in the sample with the one from the secret created in the previous step. <pre><code>curl -o /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml https://raw.githubusercontent.com/openstack-k8s-operators/openstack-operator/main/config/samples/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml\nFSID=$(oc get secret ceph-conf-files -o json | jq -r '.data.\"ceph.conf\"' | base64 -d | grep fsid | sed -e 's/fsid = //') &amp;&amp; echo $FSID\nsed -i \"s/_FSID_/${FSID}/\" /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml\noc apply -f /tmp/core_v1beta1_openstackcontrolplane_network_isolation_ceph.yaml\n</code></pre></p> <p>A NG control plane which uses the same Ceph backend should now be functional. If you create a test image on the NG system to confirm it works from the configuration above, be sure to read the warning in the next section.</p> <p>Before beginning adoption testing or development you may wish to deploy an EDPM node as described in the following section.</p>"},{"location":"contributing/development_environment/#warning-about-two-openstacks-and-one-ceph","title":"Warning about two OpenStacks and one Ceph","text":"<p>Though workloads can be created in the NG deployment to test, be careful not to confuse them with workloads from the Wallaby cluster to be migrated. The following scenario is now possible.</p> <p>A Glance image exists on the Wallaby OpenStack to be adopted. <pre><code>[stack@standalone standalone]$ export OS_CLOUD=standalone\n[stack@standalone standalone]$ openstack image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| 33a43519-a960-4cd0-a593-eca56ee553aa | cirros | active |\n+--------------------------------------+--------+--------+\n[stack@standalone standalone]$\n</code></pre> If you now create an image with the NG cluster, then a Glance image will exsit on the NG OpenStack which will adopt the workloads of the wallaby. <pre><code>[fultonj@hamfast ng]$ export OS_CLOUD=default\n[fultonj@hamfast ng]$ export OS_PASSWORD=12345678\n[fultonj@hamfast ng]$ openstack image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| 4ebccb29-193b-4d52-9ffd-034d440e073c | cirros | active |\n+--------------------------------------+--------+--------+\n[fultonj@hamfast ng]$\n</code></pre> Both Glance images are stored in the same Ceph pool. <pre><code>[stack@standalone standalone]$ sudo cephadm shell -- rbd -p images ls -l\nInferring fsid 7133115f-7751-5c2f-88bd-fbff2f140791\nUsing recent ceph image quay.rdoproject.org/tripleowallabycentos9/daemon@sha256:aa259dd2439dfaa60b27c9ebb4fb310cdf1e8e62aa7467df350baf22c5d992d8\nNAME                                       SIZE     PARENT  FMT  PROT  LOCK\n33a43519-a960-4cd0-a593-eca56ee553aa         273 B            2\n33a43519-a960-4cd0-a593-eca56ee553aa@snap    273 B            2  yes\n4ebccb29-193b-4d52-9ffd-034d440e073c       112 MiB            2\n4ebccb29-193b-4d52-9ffd-034d440e073c@snap  112 MiB            2  yes\n[stack@standalone standalone]$\n</code></pre> However, as far as each Glance service is concerned each has one image. Thus, in order to avoid confusion during adoption the test Glance image on the NG OpenStack should be deleted. <pre><code>openstack image delete 4ebccb29-193b-4d52-9ffd-034d440e073c\n</code></pre> Connecting the NG OpenStack to the existing Ceph cluster is part of the adoption procedure so that the data migration can be minimized but understand the implications of the above example.</p>"},{"location":"contributing/development_environment/#deploy-edpm-compute-1","title":"Deploy edpm-compute-1","text":"<p>edpm-compute-0 is not available as a standard EDPM system to be managed by edpm-ansible or dataplane-operator because it hosts the wallaby deployment which will be adopted and after adoption it will only host the Ceph server.</p> <p>Use the install_yamls devsetup to create additional virtual machines and be sure that the <code>EDPM_COMPUTE_SUFFIX</code> is set to <code>1</code> or greater. Do not set <code>EDPM_COMPUTE_SUFFIX</code> to <code>0</code> or you could delete the Wallaby system created in the previous section.</p> <p>When deploying EDPM nodes add an <code>extraMounts</code> like the following in the <code>OpenStackDataPlane</code> CR <code>nodeTemplate</code> so that they will be configured to use the same Ceph cluster.</p> <pre><code>    edpm-compute:\n      nodeTemplate:\n        extraMounts:\n        - extraVolType: Ceph\n          volumes:\n          - name: ceph\n            secret:\n              secretName: ceph-conf-files\n          mounts:\n          - name: ceph\n            mountPath: \"/etc/ceph\"\n            readOnly: true\n</code></pre> <p>A NG data plane which uses the same Ceph backend should now be functional. Be careful about not confusing new workloads to test the NG OpenStack with the Wallaby OpenStack as described in the previous section.</p>"},{"location":"contributing/development_environment/#begin-adoption-testing-or-development","title":"Begin Adoption Testing or Development","text":"<p>We should now have:</p> <ul> <li>An NG glance service based on Antelope running on CRC</li> <li>An TripleO-deployed glance serviced running on edpm-compute-0</li> <li>Both services have the same Ceph backend</li> <li>Each service has their own independent database</li> </ul> <p>An environment above is assumed to be available in the Glance Adoption documentation. You may now follow other Data Plane Adoption procedures described in the documentation. The same pattern can be applied to other services.</p>"},{"location":"contributing/documentation/","title":"Contributing to documentation","text":""},{"location":"contributing/documentation/#rendering-documentation-locally","title":"Rendering documentation locally","text":"<p>Install docs build requirements into virtualenv:</p> <pre><code>python3 -m venv local/docs-venv\nsource local/docs-venv/bin/activate\npip install -r docs/doc_requirements.txt\n</code></pre> <p>Serve docs site on localhost:</p> <pre><code>mkdocs serve\n</code></pre> <p>Click the link it outputs. As you save changes to files modified in your editor, the browser will automatically show the new content.</p>"},{"location":"contributing/documentation/#patterns-and-tips-for-contributing-to-documentation","title":"Patterns and tips for contributing to documentation","text":"<ul> <li> <p>Pages concerning individual components/services should make sense in   the context of the broader adoption procedure. While adopting a   service in isolation is an option for developers, let's write the   documentation with the assumption the adoption procedure is being   done in full, going step by step (one doc after another).</p> </li> <li> <p>The procedure should be written with production use in mind. This   repository could be used as a starting point for product   technical documentation. We should not tie the documentation to   something that wouldn't translate well from dev envs to production.</p> <ul> <li>This includes not assuming that the source environment is   Standalone, and the destination is CRC. We can provide examples for   Standalone/CRC, but it should be possible to use the procedure   with fuller environments in a way that is obvious from the docs.</li> </ul> </li> <li> <p>If possible, try to make code snippets copy-pastable. Use shell   variables if the snippets should be parametrized. Use <code>oc</code> rather   than <code>kubectl</code> in snippets.</p> </li> <li> <p>Focus on the \"happy path\" in the docs as much as possible,   troubleshooting info can go into the Troubleshooting page, or   alternatively a troubleshooting section at the end of the document,   visibly separated from the main procedure.</p> </li> <li> <p>The full procedure will inevitably happen to be quite long, so let's   try to be concise in writing to keep the docs consumable (but not to   a point of making things difficult to understand or omitting   important things).</p> </li> </ul>"},{"location":"contributing/tests/","title":"Tests","text":""},{"location":"contributing/tests/#test-suite-information","title":"Test suite information","text":"<p>The adoption docs repository also includes a test suite for Adoption. Currently, only one test target is defined:</p> <ul> <li><code>minimal</code> - a minimal test scenario, the eventual set of services in   this scenario should be the \"core\" services needed to launch a VM   (without Ceph, to keep environment size requirements small and make   it easy to set up).</li> </ul> <p>We can add more scenarios as we go (e.g. one that includes Ceph).</p>"},{"location":"contributing/tests/#running-the-tests","title":"Running the tests","text":"<p>The interface between the execution infrastructure and the test suite is an Ansible inventory and variables files. Inventory and variable samples are provided. To run the tests, follow this procedure:</p> <ul> <li> <p>Create <code>tests/inventory.yaml</code> file by copying and editing one of the   included samples (e.g. <code>tests/inventory.sample-crc-vagrant.yaml</code>) to   provide values valid in your environment.</p> </li> <li> <p>Create <code>tests/vars.yaml</code> and <code>tests/secrets.yaml</code>, likewise by   copying and editing the included samples (<code>tests/vars.sample.yaml</code>,   <code>tests/secrets.sample.yaml</code>).</p> </li> <li> <p>Run <code>make test-minimal</code>.</p> </li> </ul>"},{"location":"openstack/backend_services_deployment/","title":"Backend services deployment","text":"<p>The following instructions create OpenStackControlPlane CR with MariaDB and RabbitMQ deployed, and all the other services disabled. This will be the foundation of the podified control plane.</p> <p>In subsequent steps, we'll import the original databases and then add podified OpenStack control plane services.</p>"},{"location":"openstack/backend_services_deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The cloud which we want to adopt is up and running. It's on   OpenStack Wallaby release.</p> </li> <li> <p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is   not deployed.</p> </li> </ul> <p>For developer/CI environments, the openstack operator can be deployed   by running <code>make openstack</code> inside   install_yamls   repo.</p> <p>For production environments, the deployment method will likely be   different.</p> <ul> <li>There are free PVs available to be claimed (for MariaDB and RabbitMQ).</li> </ul> <p>For developer/CI environments driven by install_yamls, make sure   you've run <code>make crc_storage</code>.</p>"},{"location":"openstack/backend_services_deployment/#variables","title":"Variables","text":"<ul> <li>Set the desired admin password for the podified deployment. This can   be the original deployment's admin password or something else.</li> </ul> <pre><code>ADMIN_PASSWORD=SomePassword\n</code></pre> <ul> <li>Set service password variables to match the original deployment.   Database passwords can differ in podified environment, but   synchronizing the service account passwords is a required step.</li> </ul> <p>E.g. in developer environments with TripleO Standalone, the   passwords can be extracted like this:</p> <pre><code>CINDER_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')\nGLANCE_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')\nIRONIC_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')\nNEUTRON_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')\nNOVA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')\nOCTAVIA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')\nPLACEMENT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')\n</code></pre>"},{"location":"openstack/backend_services_deployment/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/backend_services_deployment/#procedure-backend-services-deployment","title":"Procedure - backend services deployment","text":"<ul> <li>Create OSP secret.</li> </ul> <p>The procedure for this will vary, but in developer/CI environments   we use install_yamls:</p> <pre><code># in install_yamls\nmake input\n</code></pre> <ul> <li>If the <code>$ADMIN_PASSWORD</code> is different than the already set password   in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>   correspondingly:</li> </ul> <pre><code>oc set data secret/osp-secret \"AdminPassword=$ADMIN_PASSWORD\"\n</code></pre> <ul> <li>Set service account passwords in <code>osp-secret</code> to match the service   account passwords from the original deployment:</li> </ul> <pre><code>oc set data secret/osp-secret \"CinderPassword=$CINDER_PASSWORD\"\noc set data secret/osp-secret \"GlancePassword=$GLANCE_PASSWORD\"\noc set data secret/osp-secret \"IronicPassword=$IRONIC_PASSWORD\"\noc set data secret/osp-secret \"NeutronPassword=$NEUTRON_PASSWORD\"\noc set data secret/osp-secret \"NovaPassword=$NOVA_PASSWORD\"\noc set data secret/osp-secret \"OctaviaPassword=$OCTAVIA_PASSWORD\"\noc set data secret/osp-secret \"PlacementPassword=$PLACEMENT_PASSWORD\"\n</code></pre> <ul> <li>Deploy OpenStackControlPlane. Make sure to only enable MariaDB and   RabbitMQ services. All other services must be disabled.</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: core.openstack.org/v1beta1\nkind: OpenStackControlPlane\nmetadata:\nname: openstack\nspec:\nsecret: osp-secret\nstorageClass: local-storage\n\nkeystone:\nenabled: false\ntemplate: {}\n\nmariadb:\ntemplates:\nopenstack:\ncontainerImage: quay.io/podified-antelope-centos9/openstack-mariadb:current-podified\nstorageRequest: 500M\n\nrabbitmq:\ntemplates:\nrabbitmq:\nreplicas: 1\nrabbitmq-cell1:\nreplicas: 1\n\nplacement:\nenabled: false\ntemplate: {}\n\nglance:\nenabled: false\ntemplate:\nglanceAPIInternal: {}\nglanceAPIExternal: {}\n\ncinder:\nenabled: false\ntemplate:\ncinderAPI: {}\ncinderScheduler: {}\ncinderBackup: {}\ncinderVolumes: {}\n\novn:\nenabled: false\ntemplate: {}\n\novs:\nenabled: false\ntemplate:\nexternal-ids: {}\n\nneutron:\nenabled: false\ntemplate: {}\n\nnova:\nenabled: false\ntemplate: {}\n\nironic:\nenabled: false\ntemplate:\nironicConductors: []\n\nmanila:\nenabled: false\ntemplate:\nmanilaAPI: {}\nmanilaScheduler: {}\nmanilaShares: {}\nEOF\n</code></pre>"},{"location":"openstack/backend_services_deployment/#post-checks","title":"Post-checks","text":"<ul> <li>Check that MariaDB is running.</li> </ul> <pre><code>oc get pod mariadb-openstack -o jsonpath='{.status.phase}{\"\\n\"}'\n</code></pre>"},{"location":"openstack/cinder_adoption/","title":"Cinder adoption","text":""},{"location":"openstack/cinder_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, the service databases   must already be imported into the podified MariaDB.</li> </ul>"},{"location":"openstack/cinder_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/cinder_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/cinder_adoption/#procedure-cinder-adoption","title":"Procedure - Cinder adoption","text":"<ul> <li>Patch OpenStackControlPlane to deploy Cinder:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  cinder:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      secret: osp-secret\n      cinderAPI:\n        replicas: 1\n        containerImage: quay.io/tripleozedcentos9/openstack-cinder-api:current-tripleo\n      cinderScheduler:\n        replicas: 1\n        containerImage: quay.io/tripleozedcentos9/openstack-cinder-scheduler:current-tripleo\n      cinderBackup:\n        replicas: 0 # backend needs to be configured\n        containerImage: quay.io/tripleozedcentos9/openstack-cinder-backup:current-tripleo\n      cinderVolumes:\n        volume1:\n          containerImage: quay.io/tripleozedcentos9/openstack-cinder-volume:current-tripleo\n          replicas: 0 # backend needs to be configured\n'\n</code></pre>"},{"location":"openstack/cinder_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that Cinder endpoints are defined and pointing to the podified   FQDNs:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep cinder\nopenstack volume type list\n</code></pre>"},{"location":"openstack/edpm_adoption/","title":"EDPM adoption","text":""},{"location":"openstack/edpm_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed.</li> </ul>"},{"location":"openstack/edpm_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/edpm_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/edpm_adoption/#procedure-edpm-adoption","title":"Procedure - EDPM adoption","text":"<ul> <li>Create a ssh authentication secret for the EDPM nodes:</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\n  apiVersion: v1\n  kind: Secret\n  metadata:\n      name: dataplane-adoption-secret\n      namespace: openstack\n  data:\n      ssh-privatekey: |\n  $(cat {{ edpm_privatekey_path }} | base64 | sed 's/^/        /')\n  EOF\n</code></pre> <ul> <li>Stop the nova services.</li> </ul> <pre><code># Update the services list to be stopped\n\nServicesToStop=(\"tripleo_nova_api_cron.service\"\n\"tripleo_nova_api.service\"\n\"tripleo_nova_compute.service\"\n\"tripleo_nova_conductor.service\"\n\"tripleo_nova_libvirt.target\"\n\"tripleo_nova_metadata.service\"\n\"tripleo_nova_migration_target.service\"\n\"tripleo_nova_scheduler.service\"\n\"tripleo_nova_virtlogd_wrapper.service\"\n\"tripleo_nova_virtnodedevd.service\"\n\"tripleo_nova_virtproxyd.service\"\n\"tripleo_nova_virtqemud.service\"\n\"tripleo_nova_virtsecretd.service\"\n\"tripleo_nova_virtstoraged.service\"\n\"tripleo_nova_vnc_proxy.service\")\n\necho \"Stopping nova services\"\n\nfor service in ${ServicesToStop[*]}; do\necho \"Stopping the $service in each controller node\"\n$CONTROLLER1_SSH sudo systemctl stop $service\n$CONTROLLER2_SSH sudo systemctl stop $service\n$CONTROLLER3_SSH sudo systemctl stop $service\ndone\n</code></pre> <ul> <li>Deploy OpenStackDataPlaneService(s):</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\n  apiVersion: dataplane.openstack.org/v1beta1\n  kind: OpenStackDataPlaneService\n  metadata:\n    labels:\n      app.kubernetes.io/name: openstackdataplaneservice\n      app.kubernetes.io/instance: openstackdataplaneservice-sample\n      app.kubernetes.io/part-of: dataplane-operator\n      app.kubernetes.io/managed-by: kustomize\n      app.kubernetes.io/created-by: dataplane-operator\n    name: configure-network\n  spec:\n    label: dataplane-deployment-configure-network\n    role:\n\n      name: \"Deploy EDPM Network\"\n      hosts: \"all\"\n      strategy: \"linear\"\n      tasks:\n        - name: \"Install edpm_bootstrap\"\n          import_role:\n            name: \"osp.edpm.edpm_bootstrap\"\n            tasks_from: \"bootstrap.yml\"\n          tags:\n            - \"edpm_bootstrap\"\n        - name: \"Grow volumes\"\n          import_role:\n            name: \"osp.edpm.edpm_growvols\"\n            tasks_from: \"main.yml\"\n          tags:\n            - \"edpm_growvols\"\n        - name: \"Install edpm_kernel\"\n          import_role:\n            name: \"osp.edpm.edpm_kernel\"\n            tasks_from: \"main.yml\"\n          tags:\n            - \"edpm_kernel\"\n        - name: \"Import edpm_tuned\"\n          import_role:\n            name: \"osp.edpm.edpm_tuned\"\n            tasks_from: \"main.yml\"\n          tags:\n            - \"edpm_tuned\"\n        - name: \"Configure Kernel Args\"\n          import_role:\n            name: \"osp.edpm.edpm_kernel\"\n            tasks_from: \"kernelargs.yml\"\n          tags:\n            - \"edpm_kernel\"\n        - name: \"Configure Hosts Entries\"\n          import_role:\n            name: \"osp.edpm.edpm_hosts_entries\"\n            tasks_from: \"main.yml\"\n          tags:\n            - \"edpm_hosts_entries\"\n        - name: \"import edpm_network_config\"\n          import_role:\n            name: \"osp.edpm.edpm_network_config\"\n            tasks_from: \"main.yml\"\n          tags:\n            - \"edpm_network_config\"\n  EOF\n</code></pre> <ul> <li>Deploy OpenStackDataPlane:</li> </ul> <p><pre><code>oc apply -f - &lt;&lt;EOF\n  apiVersion: dataplane.openstack.org/v1beta1\n  kind: OpenStackDataPlane\n  metadata:\n    name: openstack\n    namespace: openstack\n  spec:\n    deployStrategy:\n      deploy: true\n    nodes:\n      standalone:\n        ansibleHost: {{ edpm_node_ip }}\n        deployStrategy:\n          deploy: false\n        hostName: standalone\n        node:\n          ansibleSSHPrivateKeySecret: dataplane-adoption-secret\n          ansibleVars: |\n            ctlplane_ip: {{ edpm_node_ip }}\n            internal_api_ip: 172.17.0.100\n            storage_ip: 172.18.0.100\n            tenant_ip: 172.10.0.100\n            fqdn_internal_api: '{{'{{ ansible_fqdn }}'}}'\n        openStackAnsibleEERunnerImage: quay.io/openstack-k8s-operators/openstack-ansibleee-runner:latest\n        role: edpmadoption\n    roles:\n      edpmadoption:\n        deployStrategy:\n          deploy: false\n        networkAttachments:\n          - ctlplane\n          - internalapi\n          - storage\n          - tenant\n        services:\n          - configure-network\n        env:\n          - name: ANSIBLE_FORCE_COLOR\n            value: \"True\"\n          - name: ANSIBLE_ENABLE_TASK_DEBUGGER\n            value: \"True\"\n          - name: ANSIBLE_VERBOSITY\n            value: \"2\"\n        nodeTemplate:\n          ansiblePort: 22\n          ansibleSSHPrivateKeySecret: dataplane-adoption-secret\n          ansibleUser: root\n          ansibleVars: |\n            service_net_map:\n              nova_api_network: internal_api\n              nova_libvirt_network: internal_api\n            # edpm_network_config\n            # Default nic config template for a EDPM compute node\n            # These vars are edpm_network_config role vars\n            edpm_network_config_template: templates/single_nic_vlans/single_nic_vlans.j2\n            edpm_network_config_hide_sensitive_logs: false\n            # These vars are for the network config templates themselves and are\n            # considered EDPM network defaults.\n            neutron_physical_bridge_name: br-ctlplane\n            neutron_public_interface_name: eth1\n            ctlplane_mtu: 1500\n            ctlplane_subnet_cidr: 24\n            ctlplane_gateway_ip: 192.168.121.1\n            ctlplane_host_routes:\n            - ip_netmask: 0.0.0.0/0\n              next_hop: 192.168.121.1\n            external_mtu: 1500\n            external_vlan_id: 44\n            external_cidr: '24'\n            external_host_routes: []\n            internal_api_mtu: 1500\n            internal_api_vlan_id: 20\n            internal_api_cidr: '24'\n            internal_api_host_routes: []\n            storage_mtu: 1500\n            storage_vlan_id: 21\n            storage_cidr: '24'\n            storage_host_routes: []\n            tenant_mtu: 1500\n            tenant_vlan_id: 22\n            tenant_cidr: '24'\n            tenant_host_routes: []\n            role_networks:\n            - InternalApi\n            - Storage\n            - Tenant\n            networks_lower:\n              External: external\n              InternalApi: internal_api\n              Storage: storage\n              Tenant: tenant\n            # edpm_nodes_validation\n            edpm_nodes_validation_validate_controllers_icmp: false\n            edpm_nodes_validation_validate_gateway_icmp: false\n\n            edpm_ovn_metadata_agent_DEFAULT_transport_url: rabbit://default_user_-secret@rabbitmq.openstack.svc:5672\n            edpm_ovn_metadata_agent_metadata_agent_ovn_ovn_sb_connection: tcp:172.17.0.31:6642\n            edpm_ovn_metadata_agent_metadata_agent_DEFAULT_nova_metadata_host: 127.0.0.1\n            edpm_ovn_metadata_agent_metadata_agent_DEFAULT_metadata_proxy_shared_secret: 12345678\n            edpm_ovn_metadata_agent_DEFAULT_bind_host: 127.0.0.1\n            edpm_chrony_ntp_servers:\n            - clock.corp.redhat.com\n\n            ctlplane_dns_nameservers:\n            - 192.168.121.1\n            dns_search_domains: []\n            edpm_ovn_dbs:\n            - 172.17.0.31\n\n            edpm_ovn_controller_agent_image: quay.io/podified-antelope-centos9/openstack-ovn-controller:current-podified\n            edpm_iscsid_image: quay.io/podified-antelope-centos9/openstack-iscsid:current-podified\n            edpm_logrotate_crond_image: quay.io/podified-antelope-centos9/openstack-cron:current-podified\n            edpm_nova_compute_container_image: quay.io/podified-antelope-centos9/openstack-nova-compute:current-podified\n            edpm_nova_libvirt_container_image: quay.io/podified-antelope-centos9/openstack-nova-libvirt:current-podified\n            edpm_ovn_metadata_agent_image: quay.io/podified-antelope-centos9/openstack-neutron-metadata-agent-ovn:current-podified\n\n            gather_facts: false\n            enable_debug: false\n            verbosity: 4\n            # edpm firewall, change the allowed CIDR if needed\n            edpm_sshd_configure_firewall: true\n            edpm_sshd_allowed_ranges: ['192.168.122.0/24']\n            # SELinux module\n            edpm_selinux_mode: permissive\n            edpm_hosts_entries_undercloud_hosts_entries: []\n            # edpm_hosts_entries role\n            edpm_hosts_entries_extra_hosts_entries:\n            - 172.17.0.80 glance-internal.openstack.svc neutron-internal.openstack.svc cinder-internal.openstack.svc nova-internal.openstack.svc placement-internal.openstack.svc keystone-internal.openstack.svc\n            - 172.17.0.85 rabbitmq.openstack.svc\n            - 172.17.0.86 rabbitmq-cell1.openstack.svc\n            edpm_hosts_entries_vip_hosts_entries: []\n            hosts_entries: []\n            hosts_entry: []\n          managed: false\n          managementNetwork: ctlplane\n        openStackAnsibleEERunnerImage: quay.io/openstack-k8s-operators/openstack-ansibleee-runner:latest\n  EOF\n</code></pre> Note: Role vars will be inherited by nodes, more details here</p>"},{"location":"openstack/edpm_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that ansible jobs are running:</li> </ul> <pre><code>while true; do oc logs -f `oc get pods | grep dataplane-deployment | grep Running| cut -d ' ' -f1` 2&gt;/dev/null || echo -n .; sleep 1; done\n</code></pre>"},{"location":"openstack/glance_adoption/","title":"Glance adoption","text":"<p>Adopting Glance means that an existing <code>OpenStackControlPlane</code> CR, where Glance is supposed to be disabled, should be patched to start the service with the configuration parameters provided by the source environment.</p> <p>When the procedure is over, the expectation is to see the <code>GlanceAPI</code> service up and running: the <code>Keystone endpoints</code> should be updated and the same backend of the source Cloud will be available. If the conditions above are met, the adoption is considered concluded.</p> <p>This guide also assumes that:</p> <ol> <li>A <code>TripleO</code> environment (the source Cloud) is running on one side;</li> <li>A <code>SNO</code> / <code>CodeReadyContainers</code> is running on the other side;</li> <li>(optional) an internal/external <code>Ceph</code> cluster is reachable by both <code>crc</code> and <code>TripleO</code></li> </ol>"},{"location":"openstack/glance_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, MariaDB and Keystone   should be already adopted.</li> </ul>"},{"location":"openstack/glance_adoption/#pre-check","title":"Pre-check","text":"<p>On the source Cloud, check that the service is active and works as expected, and list the existing images:</p> <pre><code>(openstack)$ image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n</code></pre>"},{"location":"openstack/glance_adoption/#procedure-glance-adoption","title":"Procedure - Glance adoption","text":"<p>As already done for Keystone, the Glance Adoption follows the same pattern.</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n      glanceAPIExternal:\n        containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n'\n</code></pre> <p>However, if a Ceph backend is used, the <code>customServiceConfig</code> parameter should be used to inject the right configuration to the <code>GlanceAPI</code> instance.</p> <p>Make sure the Ceph-related secret exists in the <code>openstack</code> namespace:</p> <pre><code>$ oc get secrets | grep ceph\nceph-conf-files\n</code></pre> <p>If it doesn't exist, create a <code>Secret</code> which contains the <code>Cephx</code> key and Ceph configuration file so that the Glance pod created by the operator can mount those files in <code>/etc/ceph</code>.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ceph-client-conf\n  namespace: openstack\nstringData:\n  ceph.client.openstack.keyring: |\n    [client.openstack]\n        key = &lt;secret key&gt;\n        caps mgr = \"allow *\"\n        caps mon = \"profile rbd\"\n        caps osd = \"profile rbd pool=images\"\n  ceph.conf: |\n    [global]\n    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4\n    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4\n</code></pre> <p>This secret will be used in the <code>extraVolumes</code> parameters to propagate the files to the <code>GlanceAPI</code> pods (both internal and external).</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n      customServiceConfig: |\n        [DEFAULT]\n        enabled_backends=default_backend:rbd\n        [glance_store]\n        default_backend=default_backend\n        [default_backend]\n        rbd_store_ceph_conf=/etc/ceph/ceph.conf\n        rbd_store_user=openstack\n        rbd_store_pool=images\n        store_description=Ceph glance store backend.\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n      glanceAPIExternal:\n        containerImage: quay.io/podified-antelope-centos9/openstack-glance-api:current-podified\n  extraMounts:\n    - extraVol:\n      - propagation:\n        - Glance\n        extraVolType: Ceph\n        volumes:\n        - name: ceph\n          projected:\n            sources:\n            - secret:\n                name: ceph-conf-files\n        mounts:\n        - name: ceph\n          mountPath: \"/etc/ceph\"\n          readOnly: true\n'\n</code></pre>"},{"location":"openstack/glance_adoption/#post-checks","title":"Post-checks","text":""},{"location":"openstack/glance_adoption/#test-the-glance-service-from-the-openstack-cli","title":"Test the glance service from the OpenStack CLI","text":"<p>Inspect the resulting glance pods:</p> <pre><code>sh-5.1# cat /etc/glance/glance.conf.d/01-custom.conf\n\n[DEFAULT]\nenabled_backends=default_backend:rbd\n[glance_store]\ndefault_backend=default_backend\n[default_backend]\nrbd_store_ceph_conf=/etc/ceph/ceph.conf\nrbd_store_user=openstack\nrbd_store_pool=images\nstore_description=Ceph glance store backend.\n\nsh-5.1# ls /etc/ceph/ceph*\n/etc/ceph/ceph.client.openstack.keyring  /etc/ceph/ceph.conf\n</code></pre> <p>Ceph secrets are properly mounted, at this point let's move to the OpenStack CLI and check the service is active and the endpoints are properly updated.</p> <pre><code>(openstack)$ service list | grep image\n\n| fc52dbffef36434d906eeb99adfc6186 | glance    | image        |\n\n(openstack)$ endpoint list | grep image\n\n| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |\n| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |\n| 709859219bc24ab9ac548eab74ad4dd5 | regionOne | glance       | image        | True    | admin     | http://glance-admin-openstack.apps-crc.testing      |\n</code></pre> <p>Check the images that we previously listed in the source Cloud are available in the adopted service:</p> <pre><code>(openstack)$ image list\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n</code></pre>"},{"location":"openstack/glance_adoption/#image-upload","title":"Image upload","text":"<p>We can test that an image can be created on from the adopted service.</p> <pre><code>(openstack)$ export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\n(openstack)$ export OS_CLOUD=adopted\n(openstack)$ curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img\n    qemu-img convert -O raw /tmp/cirros-0.5.2-x86_64-disk.img /tmp/cirros-0.5.2-x86_64-disk.img.raw\n    openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img.raw cirros2\n    openstack image list\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   273  100   273    0     0   1525      0 --:--:-- --:--:-- --:--:--  1533\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 15.5M  100 15.5M    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M\n\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| Field            | Value                                                                                                                                      |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| container_format | bare                                                                                                                                       |\n| created_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| disk_format      | raw                                                                                                                                        |\n| file             | /v2/images/46a3eac1-7224-40bc-9083-f2f0cd122ba4/file                                                                                       |\n| id               | 46a3eac1-7224-40bc-9083-f2f0cd122ba4                                                                                                       |\n| min_disk         | 0                                                                                                                                          |\n| min_ram          | 0                                                                                                                                          |\n| name             | cirros                                                                                                                                     |\n| owner            | 9f7e8fdc50f34b658cfaee9c48e5e12d                                                                                                           |\n| properties       | os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/cirros', owner_specified.openstack.sha256='' |\n| protected        | False                                                                                                                                      |\n| schema           | /v2/schemas/image                                                                                                                          |\n| status           | queued                                                                                                                                     |\n| tags             |                                                                                                                                            |\n| updated_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| visibility       | shared                                                                                                                                     |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| 46a3eac1-7224-40bc-9083-f2f0cd122ba4 | cirros2| active |\n| c3158cad-d50b-452f-bec1-f250562f5c1f | cirros | active |\n+--------------------------------------+--------+--------+\n\n\n(openstack)$ oc rsh ceph\nsh-4.4$ ceph -s\nr  cluster:\n    id:     432d9a34-9cee-4109-b705-0c59e8973983\n    health: HEALTH_OK\n\n  services:\n    mon: 1 daemons, quorum a (age 4h)\n    mgr: a(active, since 4h)\n    osd: 1 osds: 1 up (since 4h), 1 in (since 4h)\n\n  data:\n    pools:   5 pools, 160 pgs\n    objects: 46 objects, 224 MiB\n    usage:   247 MiB used, 6.8 GiB / 7.0 GiB avail\n    pgs:     160 active+clean\n\nsh-4.4$ rbd -p images ls\n46a3eac1-7224-40bc-9083-f2f0cd122ba4\nc3158cad-d50b-452f-bec1-f250562f5c1f\n</code></pre>"},{"location":"openstack/keystone_adoption/","title":"Keystone adoption","text":""},{"location":"openstack/keystone_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, the service databases   must already be imported into the podified MariaDB.</li> </ul>"},{"location":"openstack/keystone_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/keystone_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/keystone_adoption/#procedure-keystone-adoption","title":"Procedure - Keystone adoption","text":"<ul> <li>Patch OpenStackControlPlane to deploy Keystone:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  keystone:\n    enabled: true\n    template:\n      secret: osp-secret\n      containerImage: quay.io/podified-antelope-centos9/openstack-keystone:current-podified\n      databaseInstance: openstack\n'\n</code></pre> <ul> <li>Create a clouds.yaml file to talk to adopted Keystone:</li> </ul> <pre><code>cat &gt; clouds-adopted.yaml &lt;&lt;EOF\nclouds:\n  adopted:\n    auth:\n      auth_url: http://keystone-public-openstack.apps-crc.testing\n      password: $ADMIN_PASSWORD\n      project_domain_name: Default\n      project_name: admin\n      user_domain_name: Default\n      username: admin\n    cacert: ''\n    identity_api_version: '3'\n    region_name: regionOne\n    volume_api_version: '3'\nEOF\n</code></pre> <ul> <li>Clean up old services and endpoints that still point to the old   control plane (everything except Keystone service and endpoints):</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep keystone | awk '/admin/{ print $2; }' | xargs openstack endpoint delete || true\n\nopenstack endpoint list | awk '/ cinderv3 /{ print $2; }' | xargs openstack endpoint delete || true\nopenstack endpoint list | awk '/ glance /{ print $2; }' | xargs openstack endpoint delete || true\nopenstack endpoint list | awk '/ neutron /{ print $2; }' | xargs openstack endpoint delete || true\nopenstack endpoint list | awk '/ nova /{ print $2; }' | xargs openstack endpoint delete || true\nopenstack endpoint list | awk '/ placement /{ print $2; }' | xargs openstack endpoint delete || true\nopenstack endpoint list | awk '/ swift /{ print $2; }' | xargs openstack endpoint delete || true\n</code></pre>"},{"location":"openstack/keystone_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that Keystone endpoints are defined and pointing to the podified   FQDNs:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep keystone\n</code></pre>"},{"location":"openstack/mariadb_copy/","title":"MariaDB data copy","text":"<p>This document describes how to move the databases from the original OpenStack deployment to the MariaDB instances in the OpenShift cluster.</p>"},{"location":"openstack/mariadb_copy/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Make sure the previous Adoption steps have been performed successfully.</p> </li> <li> <p>The OpenStackControlPlane resource must be already created at this point.</p> </li> <li> <p>Podified MariaDB and RabbitMQ are running. No other podified     control plane services are running.</p> </li> <li> <p>There must be network routability between:</p> <ul> <li> <p>The adoption host and the original MariaDB.</p> </li> <li> <p>The adoption host and the podified MariaDB.</p> </li> <li> <p>Note that this routability requirement may change in the   future, e.g. we may require routability from the original MariaDB to   podified MariaDB.</p> </li> </ul> </li> </ul>"},{"location":"openstack/mariadb_copy/#variables","title":"Variables","text":"<p>Define the shell variables used in the steps below. The values are just illustrative, use values that are correct for your environment:</p> <pre><code>PODIFIED_MARIADB_IP=$(oc get -o yaml pod mariadb-openstack | grep podIP: | awk '{ print $2; }')\nMARIADB_IMAGE=quay.io/podified-antelope-centos9/openstack-mariadb:current-podified\n\n# Use your environment's values for these:\nEXTERNAL_MARIADB_IP=192.168.24.3\nEXTERNAL_DB_ROOT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')\nPODIFIED_DB_ROOT_PASSWORD=12345678\n\n# ssh commands to reach controller machines\nCONTROLLER1_SSH=\"ssh -F ~/director_standalone/vagrant_ssh_config vagrant@standalone\"\nCONTROLLER2_SSH=\":\"\nCONTROLLER3_SSH=\":\"\n</code></pre>"},{"location":"openstack/mariadb_copy/#pre-checks","title":"Pre-checks","text":"<ul> <li>Test connection to the original DB (show databases):</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysql -h \"$EXTERNAL_MARIADB_IP\" -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre> <ul> <li>Run mysqlcheck on the original DB:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysqlcheck --all-databases -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\"\n</code></pre> <ul> <li>Test connection to podified DB (show databases):</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n    mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"openstack/mariadb_copy/#procedure-stopping-control-plane-services","title":"Procedure - stopping control plane services","text":"<p>From each controller node, it is necessary to stop the control-plane services to avoid inconsistencies in the data migrated for the data-plane adoption procedure.</p> <p>1- Connect to all the controller nodes and stop the control plane services.</p> <p>2- Stop the services.</p> <pre><code># Update the services list to be stopped\n\nServicesToStop=(\"tripleo_horizon.service\"\n\"tripleo_keystone.service\"\n\"tripleo_cinder_api.service\"\n\"tripleo_glance_api.service\"\n\"tripleo_neutron_api.service\"\n\"tripleo_nova_api.service\"\n\"tripleo_placement_api.service\")\n\necho \"Stopping the OpenStack services\"\n\nfor service in ${ServicesToStop[*]}; do\necho \"Stopping the $service in each controller node\"\n$CONTROLLER1_SSH sudo systemctl stop $service\n$CONTROLLER2_SSH sudo systemctl stop $service\n$CONTROLLER3_SSH sudo systemctl stop $service\ndone\n</code></pre> <p>3- Make sure all the services are stopped</p>"},{"location":"openstack/mariadb_copy/#procedure-data-copy","title":"Procedure - data copy","text":"<ul> <li>Create a temporary folder to store DB dumps and make sure it's the   working directory for the following steps:</li> </ul> <pre><code>mkdir ~/adoption-db\ncd ~/adoption-db\n</code></pre> <ul> <li>Create a dump of the original databases:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE bash &lt;&lt;EOF\n\nmysql -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -N -e 'show databases' | while read dbname; do\n    echo \"Exporting \\$dbname\"\n    mysqldump -h $EXTERNAL_MARIADB_IP -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" \\\n        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \\\n        --databases \"\\$dbname\" \\\n        &gt; \"\\$dbname\".sql\ndone\n\nEOF\n</code></pre> <ul> <li>Restore the databases from .sql files into the podified MariaDB:</li> </ul> <pre><code>for dbname in cinder glance keystone nova_api nova_cell0 nova ovs_neutron placement; do\n    echo \"Importing $dbname\"\n    oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n       mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" &lt; \"$dbname.sql\"\ndone\n</code></pre>"},{"location":"openstack/mariadb_copy/#post-checks","title":"Post-checks","text":"<ul> <li>Check that the databases were imported correctly:</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n   mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"openstack/other_services_adoption/","title":"Adoption of other services","text":"<p>This part of the guide adopts the remaining services that don't have a specific guide of their own. It is likely that as adoption gets developed further, services will be removed from here and put into their own guides (e.g. like Glance).</p>"},{"location":"openstack/other_services_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed.</li> </ul>"},{"location":"openstack/other_services_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/other_services_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"openstack/other_services_adoption/#procedure-adoption-of-other-services","title":"Procedure - Adoption of other services","text":"<ul> <li>Deploy the rest of the control plane services:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  # cinder:\n  #   enabled: true\n  #   template:\n  #     cinderAPI:\n  #       replicas: 1\n  #       containerImage: quay.io/podified-antelope-centos9/openstack-cinder-api:current-podified\n  #     cinderScheduler:\n  #       replicas: 1\n  #       containerImage: quay.io/podified-antelope-centos9/openstack-cinder-scheduler:current-podified\n  #     cinderBackup:\n  #       replicas: 1\n  #       containerImage: quay.io/podified-antelope-centos9/openstack-cinder-backup:current-podified\n  #     cinderVolumes:\n  #       volume1:\n  #         containerImage: quay.io/podified-antelope-centos9/openstack-cinder-volume:current-podified\n  #         replicas: 1\n\n  ovn:\n    enabled: true\n    template:\n      ovnDBCluster:\n        ovndbcluster-nb:\n          replicas: 1\n          containerImage: quay.io/podified-antelope-centos9/openstack-ovn-nb-db-server:current-podified\n          dbType: NB\n          storageRequest: 10G\n        ovndbcluster-sb:\n          replicas: 1\n          containerImage: quay.io/podified-antelope-centos9/openstack-ovn-sb-db-server:current-podified\n          dbType: SB\n          storageRequest: 10G\n      ovnNorthd:\n        replicas: 1\n        containerImage: quay.io/podified-antelope-centos9/openstack-ovn-northd:current-podified\n\n  ovs:\n    enabled: true\n    template:\n      ovsContainerImage: \"quay.io/skaplons/ovs:latest\"\n      ovnContainerImage: \"quay.io/podified-antelope-centos9/openstack-ovn-controller:current-podified\"\n      external-ids:\n        system-id: \"random\"\n        ovn-bridge: \"br-int\"\n        ovn-encap-type: \"geneve\"\n\n  neutron:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/podified-antelope-centos9/openstack-neutron-server:current-podified\n      secret: osp-secret\n\n  # nova:\n  #   enabled: true\n  #   template:\n  #     secret: osp-secret\n'\n</code></pre>"},{"location":"openstack/other_services_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that service endpoints are defined:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list\n</code></pre>"},{"location":"openstack/ovn_adoption/","title":"OVN data migration","text":"<p>This document describes how to move OVN northbound and southbound databases from the original OpenStack deployment to ovsdb-server instances running in the OpenShift cluster.</p>"},{"location":"openstack/ovn_adoption/#rationale","title":"Rationale","text":"<p>While it may be argued that the podified Neutron ML2/OVN driver and OVN northd service will reconstruct the databases on startup, the reconstruction may be time consuming on large existing clusters. The procedure below allows to speed up data migration and avoid unnecessary data plane disruptions due to incomplete OpenFlow table contents.</p>"},{"location":"openstack/ovn_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make sure the previous Adoption steps have been performed successfully.</li> <li>The OpenStackControlPlane resource must be already created at this point.</li> <li>NetworkAttachmentDefinition CRDs for the original cluster are already     defined. Specifically, openstack/internalapi network is defined.</li> <li>Podified MariaDB and RabbitMQ may already run. Neutron and OVN are not     running yet.</li> <li>Original OVN is older or equal to the podified version.</li> <li>There must be network routability between:<ul> <li>The adoption host and the original OVN.</li> <li>The adoption host and the podified OVN.</li> </ul> </li> </ul>"},{"location":"openstack/ovn_adoption/#variables","title":"Variables","text":"<p>Define the shell variables used in the steps below. The values are just illustrative, use values that are correct for your environment:</p> <pre><code>OVSDB_IMAGE=quay.io/podified-antelope-centos9/openstack-ovn-base:current-podified\nEXTERNAL_OVSDB_IP=172.17.1.49\n\n# ssh commands to reach the original controller machines\nCONTROLLER_SSH=\"ssh -F ~/director_standalone/vagrant_ssh_config vagrant@standalone\"\n\n# ssh commands to reach the original compute machines\nCOMPUTE_SSH=\"ssh -F ~/director_standalone/vagrant_ssh_config vagrant@standalone\"\n</code></pre>"},{"location":"openstack/ovn_adoption/#procedure","title":"Procedure","text":"<ul> <li>Stop OVN northd on all original cluster controllers.</li> </ul> <pre><code>${CONTROLLER_SSH} sudo systemctl stop tripleo_ovn_cluster_northd.service\n</code></pre> <ul> <li>Backup OVN databases.</li> </ul> <pre><code>client=\"podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $OVSDB_IMAGE ovsdb-client\"\n${client} backup tcp:$EXTERNAL_OVSDB_IP:6641 &gt; ovs-nb.db\n${client} backup tcp:$EXTERNAL_OVSDB_IP:6642 &gt; ovs-sb.db\n</code></pre> <ul> <li>Start podified OVN services prior to database import.</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\novn:\nenabled: true\ntemplate:\novnDBCluster:\novndbcluster-nb:\ncontainerImage: quay.io/podified-antelope-centos9/openstack-ovn-nb-db-server:current-podified\ndbType: NB\nstorageRequest: 10G\nnetworkAttachment: internalapi\novndbcluster-sb:\ncontainerImage: quay.io/podified-antelope-centos9/openstack-ovn-sb-db-server:current-podified\ndbType: SB\nstorageRequest: 10G\nnetworkAttachment: internalapi\novnNorthd:\ncontainerImage: quay.io/podified-antelope-centos9/openstack-ovn-northd:current-podified\nnetworkAttachment: internalapi\n'\n</code></pre> <ul> <li>Fetch podified OVN IP addresses.</li> </ul> <pre><code>PODIFIED_OVSDB_NB_IP=$(kubectl get po ovsdbserver-nb-0 -o jsonpath='{.metadata.annotations.k8s\\.v1\\.cni\\.cncf\\.io/networks-status}' | jq 'map(. | select(.name==\"openstack/internalapi\"))[0].ips[0]' | tr -d '\"')\nPODIFIED_OVSDB_SB_IP=$(kubectl get po ovsdbserver-sb-0 -o jsonpath='{.metadata.annotations.k8s\\.v1\\.cni\\.cncf\\.io/networks-status}' | jq 'map(. | select(.name==\"openstack/internalapi\"))[0].ips[0]' | tr -d '\"')\n</code></pre> <ul> <li>Restore database backup to podified OVN database servers.</li> </ul> <pre><code>podman run -it --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $OVSDB_IMAGE bash -c \"ovsdb-client restore tcp:$PODIFIED_OVSDB_NB_IP:6641 &lt; ovs-nb.db\"\npodman run -it --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $OVSDB_IMAGE bash -c \"ovsdb-client restore tcp:$PODIFIED_OVSDB_SB_IP:6642 &lt; ovs-sb.db\"\n</code></pre> <ul> <li>Check that podified OVN databases contain objects from backup, e.g.:</li> </ul> <pre><code>oc exec -it ovsdbserver-nb-0 -- ovn-nbctl show\noc exec -it ovsdbserver-sb-0 -- ovn-sbctl list Chassis\n</code></pre> <ul> <li>Switch ovn-remote on compute nodes to point to the new podified database.</li> </ul> <pre><code>${COMPUTE_SSH} sudo podman exec -it ovn_controller ovs-vsctl set open . external_ids:ovn-remote=tcp:$PODIFIED_OVSDB_SB_IP:6642\n</code></pre> <p>You should now see the following warning in the <code>ovn_controller</code> container logs:</p> <pre><code>2023-03-16T21:40:35Z|03095|ovsdb_cs|WARN|tcp:172.17.1.50:6642: clustered database server has stale data; trying another server\n</code></pre> <ul> <li>Reset RAFT state for all compute ovn-controller instances.</li> </ul> <pre><code>${COMPUTE_SSH} sudo podman exec -it ovn_controller ovn-appctl -t ovn-controller sb-cluster-state-reset\n</code></pre> <p>This should complete connection of the controller process to the new remote. See in logs:</p> <pre><code>2023-03-16T21:42:31Z|03134|main|INFO|Resetting southbound database cluster state\n2023-03-16T21:42:33Z|03135|reconnect|INFO|tcp:172.17.1.50:6642: connected\n</code></pre> <ul> <li>Alternatively, just restart ovn-controller on original compute nodes.</li> </ul> <pre><code>$ ${COMPUTE_SSH} sudo systemctl restart tripleo_ovn_controller.service\n</code></pre>"},{"location":"openstack/placement_adoption/","title":"Placement adoption","text":""},{"location":"openstack/placement_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, the service databases   must already be imported into the podified MariaDB.</li> </ul>"},{"location":"openstack/placement_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"openstack/placement_adoption/#procedure-placement-adoption","title":"Procedure - Placement adoption","text":"<ul> <li>Patch OpenStackControlPlane to deploy Placement:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  placement:\n    enabled: true\n    template:\n      containerImage: quay.io/podified-antelope-centos9/openstack-placement-api:current-podified\n      databaseInstance: openstack\n      secret: osp-secret\n'\n</code></pre>"},{"location":"openstack/placement_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that Placement endpoints are defined and pointing to the   podified FQDNs and that Placement API responds.</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep placement\n\n\n# Without OpenStack CLI placement plugin installed:\nPLACEMENT_PUBLIC_URL=$(openstack endpoint list -c 'Service Name' -c 'Service Type' -c URL | grep placement | grep public | awk '{ print $6; }')\ncurl \"$PLACEMENT_PUBLIC_URL\"\n\n# With OpenStack CLI placement plugin installed:\nopenstack resource class list\n</code></pre>"},{"location":"openstack/troubleshooting/","title":"Troubleshooting","text":"<p>This document contains information about various issues you might face and how to solve them.</p>"},{"location":"openstack/troubleshooting/#errimagepull-due-to-missing-authentication","title":"ErrImagePull due to missing authentication","text":"<p>The deployed containers pull the images from private containers registries that can potentially return authentication errors like:</p> <pre><code>Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\":\nrpc error: code = Unknown desc = unable to retrieve auth token: invalid\nusername/password: unauthorized: Please login to the Red Hat Registry using\nyour Customer Portal credentials.\n</code></pre> <p>An example of a failed pod:</p> <pre><code>  Normal   Scheduled       3m40s                  default-scheduler  Successfully assigned openstack/rabbitmq-server-0 to worker0\n  Normal   AddedInterface  3m38s                  multus             Add eth0 [10.101.0.41/23] from ovn-kubernetes\n  Warning  Failed          2m16s (x6 over 3m38s)  kubelet            Error: ImagePullBackOff\n  Normal   Pulling         2m5s (x4 over 3m38s)   kubelet            Pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\": rpc error: code  ... can be found here: https://access.redhat.com/RegistryAuthentication\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Error: ErrImagePull\n  Normal   BackOff         110s (x7 over 3m38s)   kubelet            Back-off pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n</code></pre> <p>To solve this issue we need to get a valid pull-secret from the official Red Hat console site, store this pull secret locally in a machine with access to the Kubernetes API (service node), and then run:</p> <pre><code>oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location.json&gt;\n</code></pre> <p>The previous command will make available the authentication information in all the cluster's compute nodes, then trigger a new pod deployment to pull the container image with:</p> <pre><code>kubectl delete pod rabbitmq-server-0 -n openstack\n</code></pre> <p>And the pod should be able to pull the image successfully.  For more information about what container registries requires what type of authentication, check the official docs.</p>"},{"location":"openstack/troubleshooting/#database-schema-version-mismatch-when-restoring-ovn-databases","title":"Database schema version mismatch when restoring OVN databases","text":"<p>You may see the following errors when trying to restore OVN databases:</p> <pre><code>ovsdb-client: backup schema has version \"7.0.0\" but database schema has version \"6.3.0\" (use --force to override differences, or \"ovsdb-client convert\" to change the schema)\novsdb-client: backup schema has version \"20.27.0\" but database schema has version \"20.23.0\" (use --force to override differences, or \"ovsdb-client convert\" to change the schema)\n</code></pre> <p>This happens because your podified cluster is running OVN version that is older than the version running in the original cluster. You may work this issue around by pulling OVN images from the original cluster registry into podified cluster.</p> <p>You may have to first enable insecure registries in your podified cluster:</p> <pre><code>oc patch image.config.openshift.io/cluster --type=merge --patch '\nspec:\n\u00a0 registrySources:\n\u00a0 \u00a0 insecureRegistries:\n\u00a0 \u00a0 - 192.168.130.1\n'\n</code></pre> <p>Then update OVN services to pull images from the original registry:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  ovn:\n    template:\n      ovnDBCluster:\n        ovndbcluster-nb:\n          containerImage: 192.168.130.1:8787/rh-osbs/rhosp17-openstack-ovn-nb-db-server:17.1_20230130.1\n        ovndbcluster-sb:\n          containerImage: 192.168.130.1:8787/rh-osbs/rhosp17-openstack-ovn-sb-db-server:17.1_20230130.1\n      ovnNorthd:\n        containerImage: 192.168.130.1:8787/rh-osbs/rhosp17-openstack-ovn-nb-db-server:17.1_20230130.1\n'\n</code></pre> <p>Now repeat database <code>ovsdb-client restore</code> commands. They should succeed.</p>"}]}